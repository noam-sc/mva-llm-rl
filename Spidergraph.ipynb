{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "rBnryXD2hSsr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig,LoraConfig, get_peft_model\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "cKI8UCkNkSJf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We just instantiate the model\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")"
      ],
      "metadata": {
        "id": "BsFCdp1Nhqus"
      },
      "execution_count": 426,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To check if it fits the configuration of our files\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q\", \"k\", \"v\", \"wi\", \"wo\", \"lm_head\"],\n",
        "    lora_dropout=0.0,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\"\n",
        ")"
      ],
      "metadata": {
        "id": "WvpCZTf2mkjn"
      },
      "execution_count": 427,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, lora_config)\n"
      ],
      "metadata": {
        "id": "8m-71HQDm5iF"
      },
      "execution_count": 428,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load(\"model.checkpoint\", map_location=\"cpu\")\n",
        "state_dict = {k.replace('_LLM_model.', '').replace('module.', ''): v for k, v in state_dict.items()}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O3IylJBf2yCy"
      },
      "execution_count": 429,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in state_dict.items():\n",
        "    print(key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZHZmAnE22kj",
        "outputId": "e5a24537-b6d1-44dd-e815-04d1c8d11b64"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_B.default.weight\n",
            "_module_functions.value.value_head_op.0.weight\n",
            "_module_functions.value.value_head_op.0.bias\n",
            "_module_functions.value.value_head_op.2.weight\n",
            "_module_functions.value.value_head_op.2.bias\n",
            "_module_functions.value.value_head_op.4.weight\n",
            "_module_functions.value.value_head_op.4.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in model.named_parameters():\n",
        "    print(key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "direiIMQ2_sW",
        "outputId": "35f16c49-b390-438f-9e66-b7970fcd2734"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base_model.model.shared.weight\n",
            "base_model.model.encoder.block.0.layer.0.SelfAttention.q.base_layer.weight\n",
            "base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.encoder.block.0.layer.0.SelfAttention.k.base_layer.weight\n",
            "base_model.model.encoder.block.0.layer.0.SelfAttention.k.lora_A.default.weight\n",
            "base_model.model.encoder.block.0.layer.0.SelfAttention.k.lora_B.default.weight\n",
            "base_model.model.encoder.block.0.layer.0.SelfAttention.v.base_layer.weight\n",
            "base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.encoder.block.0.layer.0.SelfAttention.o.weight\n",
            "base_model.model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
            "base_model.model.encoder.block.0.layer.0.layer_norm.weight\n",
            "base_model.model.encoder.block.0.layer.1.DenseReluDense.wi_0.weight\n",
            "base_model.model.encoder.block.0.layer.1.DenseReluDense.wi_1.weight\n",
            "base_model.model.encoder.block.0.layer.1.DenseReluDense.wo.base_layer.weight\n",
            "base_model.model.encoder.block.0.layer.1.DenseReluDense.wo.lora_A.default.weight\n",
            "base_model.model.encoder.block.0.layer.1.DenseReluDense.wo.lora_B.default.weight\n",
            "base_model.model.encoder.block.0.layer.1.layer_norm.weight\n",
            "base_model.model.encoder.block.1.layer.0.SelfAttention.q.base_layer.weight\n",
            "base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.encoder.block.1.layer.0.SelfAttention.k.base_layer.weight\n",
            "base_model.model.encoder.block.1.layer.0.SelfAttention.k.lora_A.default.weight\n",
            "base_model.model.encoder.block.1.layer.0.SelfAttention.k.lora_B.default.weight\n",
            "base_model.model.encoder.block.1.layer.0.SelfAttention.v.base_layer.weight\n",
            "base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.encoder.block.1.layer.0.SelfAttention.o.weight\n",
            "base_model.model.encoder.block.1.layer.0.layer_norm.weight\n",
            "base_model.model.encoder.block.1.layer.1.DenseReluDense.wi_0.weight\n",
            "base_model.model.encoder.block.1.layer.1.DenseReluDense.wi_1.weight\n",
            "base_model.model.encoder.block.1.layer.1.DenseReluDense.wo.base_layer.weight\n",
            "base_model.model.encoder.block.1.layer.1.DenseReluDense.wo.lora_A.default.weight\n",
            "base_model.model.encoder.block.1.layer.1.DenseReluDense.wo.lora_B.default.weight\n",
            "base_model.model.encoder.block.1.layer.1.layer_norm.weight\n",
            "base_model.model.encoder.block.2.layer.0.SelfAttention.q.base_layer.weight\n",
            "base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.encoder.block.2.layer.0.SelfAttention.k.base_layer.weight\n",
            "base_model.model.encoder.block.2.layer.0.SelfAttention.k.lora_A.default.weight\n",
            "base_model.model.encoder.block.2.layer.0.SelfAttention.k.lora_B.default.weight\n",
            "base_model.model.encoder.block.2.layer.0.SelfAttention.v.base_layer.weight\n",
            "base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.encoder.block.2.layer.0.SelfAttention.o.weight\n",
            "base_model.model.encoder.block.2.layer.0.layer_norm.weight\n",
            "base_model.model.encoder.block.2.layer.1.DenseReluDense.wi_0.weight\n",
            "base_model.model.encoder.block.2.layer.1.DenseReluDense.wi_1.weight\n",
            "base_model.model.encoder.block.2.layer.1.DenseReluDense.wo.base_layer.weight\n",
            "base_model.model.encoder.block.2.layer.1.DenseReluDense.wo.lora_A.default.weight\n",
            "base_model.model.encoder.block.2.layer.1.DenseReluDense.wo.lora_B.default.weight\n",
            "base_model.model.encoder.block.2.layer.1.layer_norm.weight\n",
            "base_model.model.encoder.block.3.layer.0.SelfAttention.q.base_layer.weight\n",
            "base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.encoder.block.3.layer.0.SelfAttention.k.base_layer.weight\n",
            "base_model.model.encoder.block.3.layer.0.SelfAttention.k.lora_A.default.weight\n",
            "base_model.model.encoder.block.3.layer.0.SelfAttention.k.lora_B.default.weight\n",
            "base_model.model.encoder.block.3.layer.0.SelfAttention.v.base_layer.weight\n",
            "base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.encoder.block.3.layer.0.SelfAttention.o.weight\n",
            "base_model.model.encoder.block.3.layer.0.layer_norm.weight\n",
            "base_model.model.encoder.block.3.layer.1.DenseReluDense.wi_0.weight\n",
            "base_model.model.encoder.block.3.layer.1.DenseReluDense.wi_1.weight\n",
            "base_model.model.encoder.block.3.layer.1.DenseReluDense.wo.base_layer.weight\n",
            "base_model.model.encoder.block.3.layer.1.DenseReluDense.wo.lora_A.default.weight\n",
            "base_model.model.encoder.block.3.layer.1.DenseReluDense.wo.lora_B.default.weight\n",
            "base_model.model.encoder.block.3.layer.1.layer_norm.weight\n",
            "base_model.model.encoder.block.4.layer.0.SelfAttention.q.base_layer.weight\n",
            "base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.encoder.block.4.layer.0.SelfAttention.k.base_layer.weight\n",
            "base_model.model.encoder.block.4.layer.0.SelfAttention.k.lora_A.default.weight\n",
            "base_model.model.encoder.block.4.layer.0.SelfAttention.k.lora_B.default.weight\n",
            "base_model.model.encoder.block.4.layer.0.SelfAttention.v.base_layer.weight\n",
            "base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.encoder.block.4.layer.0.SelfAttention.o.weight\n",
            "base_model.model.encoder.block.4.layer.0.layer_norm.weight\n",
            "base_model.model.encoder.block.4.layer.1.DenseReluDense.wi_0.weight\n",
            "base_model.model.encoder.block.4.layer.1.DenseReluDense.wi_1.weight\n",
            "base_model.model.encoder.block.4.layer.1.DenseReluDense.wo.base_layer.weight\n",
            "base_model.model.encoder.block.4.layer.1.DenseReluDense.wo.lora_A.default.weight\n",
            "base_model.model.encoder.block.4.layer.1.DenseReluDense.wo.lora_B.default.weight\n",
            "base_model.model.encoder.block.4.layer.1.layer_norm.weight\n",
            "base_model.model.encoder.block.5.layer.0.SelfAttention.q.base_layer.weight\n",
            "base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.encoder.block.5.layer.0.SelfAttention.k.base_layer.weight\n",
            "base_model.model.encoder.block.5.layer.0.SelfAttention.k.lora_A.default.weight\n",
            "base_model.model.encoder.block.5.layer.0.SelfAttention.k.lora_B.default.weight\n",
            "base_model.model.encoder.block.5.layer.0.SelfAttention.v.base_layer.weight\n",
            "base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.encoder.block.5.layer.0.SelfAttention.o.weight\n",
            "base_model.model.encoder.block.5.layer.0.layer_norm.weight\n",
            "base_model.model.encoder.block.5.layer.1.DenseReluDense.wi_0.weight\n",
            "base_model.model.encoder.block.5.layer.1.DenseReluDense.wi_1.weight\n",
            "base_model.model.encoder.block.5.layer.1.DenseReluDense.wo.base_layer.weight\n",
            "base_model.model.encoder.block.5.layer.1.DenseReluDense.wo.lora_A.default.weight\n",
            "base_model.model.encoder.block.5.layer.1.DenseReluDense.wo.lora_B.default.weight\n",
            "base_model.model.encoder.block.5.layer.1.layer_norm.weight\n",
            "base_model.model.encoder.block.6.layer.0.SelfAttention.q.base_layer.weight\n",
            "base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.encoder.block.6.layer.0.SelfAttention.k.base_layer.weight\n",
            "base_model.model.encoder.block.6.layer.0.SelfAttention.k.lora_A.default.weight\n",
            "base_model.model.encoder.block.6.layer.0.SelfAttention.k.lora_B.default.weight\n",
            "base_model.model.encoder.block.6.layer.0.SelfAttention.v.base_layer.weight\n",
            "base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.encoder.block.6.layer.0.SelfAttention.o.weight\n",
            "base_model.model.encoder.block.6.layer.0.layer_norm.weight\n",
            "base_model.model.encoder.block.6.layer.1.DenseReluDense.wi_0.weight\n",
            "base_model.model.encoder.block.6.layer.1.DenseReluDense.wi_1.weight\n",
            "base_model.model.encoder.block.6.layer.1.DenseReluDense.wo.base_layer.weight\n",
            "base_model.model.encoder.block.6.layer.1.DenseReluDense.wo.lora_A.default.weight\n",
            "base_model.model.encoder.block.6.layer.1.DenseReluDense.wo.lora_B.default.weight\n",
            "base_model.model.encoder.block.6.layer.1.layer_norm.weight\n",
            "base_model.model.encoder.block.7.layer.0.SelfAttention.q.base_layer.weight\n",
            "base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.encoder.block.7.layer.0.SelfAttention.k.base_layer.weight\n",
            "base_model.model.encoder.block.7.layer.0.SelfAttention.k.lora_A.default.weight\n",
            "base_model.model.encoder.block.7.layer.0.SelfAttention.k.lora_B.default.weight\n",
            "base_model.model.encoder.block.7.layer.0.SelfAttention.v.base_layer.weight\n",
            "base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.encoder.block.7.layer.0.SelfAttention.o.weight\n",
            "base_model.model.encoder.block.7.layer.0.layer_norm.weight\n",
            "base_model.model.encoder.block.7.layer.1.DenseReluDense.wi_0.weight\n",
            "base_model.model.encoder.block.7.layer.1.DenseReluDense.wi_1.weight\n",
            "base_model.model.encoder.block.7.layer.1.DenseReluDense.wo.base_layer.weight\n",
            "base_model.model.encoder.block.7.layer.1.DenseReluDense.wo.lora_A.default.weight\n",
            "base_model.model.encoder.block.7.layer.1.DenseReluDense.wo.lora_B.default.weight\n",
            "base_model.model.encoder.block.7.layer.1.layer_norm.weight\n",
            "base_model.model.encoder.final_layer_norm.weight\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.q.base_layer.weight\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.k.base_layer.weight\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.k.lora_A.default.weight\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.k.lora_B.default.weight\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.v.base_layer.weight\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.o.weight\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
            "base_model.model.decoder.block.0.layer.0.layer_norm.weight\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.base_layer.weight\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.k.base_layer.weight\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.k.lora_A.default.weight\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.k.lora_B.default.weight\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.base_layer.weight\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.o.weight\n",
            "base_model.model.decoder.block.0.layer.1.layer_norm.weight\n",
            "base_model.model.decoder.block.0.layer.2.DenseReluDense.wi_0.weight\n",
            "base_model.model.decoder.block.0.layer.2.DenseReluDense.wi_1.weight\n",
            "base_model.model.decoder.block.0.layer.2.DenseReluDense.wo.base_layer.weight\n",
            "base_model.model.decoder.block.0.layer.2.DenseReluDense.wo.lora_A.default.weight\n",
            "base_model.model.decoder.block.0.layer.2.DenseReluDense.wo.lora_B.default.weight\n",
            "base_model.model.decoder.block.0.layer.2.layer_norm.weight\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.q.base_layer.weight\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.k.base_layer.weight\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.k.lora_A.default.weight\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.k.lora_B.default.weight\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.v.base_layer.weight\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.o.weight\n",
            "base_model.model.decoder.block.1.layer.0.layer_norm.weight\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.base_layer.weight\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.k.base_layer.weight\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.k.lora_A.default.weight\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.k.lora_B.default.weight\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.base_layer.weight\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.o.weight\n",
            "base_model.model.decoder.block.1.layer.1.layer_norm.weight\n",
            "base_model.model.decoder.block.1.layer.2.DenseReluDense.wi_0.weight\n",
            "base_model.model.decoder.block.1.layer.2.DenseReluDense.wi_1.weight\n",
            "base_model.model.decoder.block.1.layer.2.DenseReluDense.wo.base_layer.weight\n",
            "base_model.model.decoder.block.1.layer.2.DenseReluDense.wo.lora_A.default.weight\n",
            "base_model.model.decoder.block.1.layer.2.DenseReluDense.wo.lora_B.default.weight\n",
            "base_model.model.decoder.block.1.layer.2.layer_norm.weight\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.q.base_layer.weight\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.k.base_layer.weight\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.k.lora_A.default.weight\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.k.lora_B.default.weight\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.v.base_layer.weight\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.o.weight\n",
            "base_model.model.decoder.block.2.layer.0.layer_norm.weight\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.base_layer.weight\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.k.base_layer.weight\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.k.lora_A.default.weight\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.k.lora_B.default.weight\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.base_layer.weight\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.o.weight\n",
            "base_model.model.decoder.block.2.layer.1.layer_norm.weight\n",
            "base_model.model.decoder.block.2.layer.2.DenseReluDense.wi_0.weight\n",
            "base_model.model.decoder.block.2.layer.2.DenseReluDense.wi_1.weight\n",
            "base_model.model.decoder.block.2.layer.2.DenseReluDense.wo.base_layer.weight\n",
            "base_model.model.decoder.block.2.layer.2.DenseReluDense.wo.lora_A.default.weight\n",
            "base_model.model.decoder.block.2.layer.2.DenseReluDense.wo.lora_B.default.weight\n",
            "base_model.model.decoder.block.2.layer.2.layer_norm.weight\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.q.base_layer.weight\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.k.base_layer.weight\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.k.lora_A.default.weight\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.k.lora_B.default.weight\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.v.base_layer.weight\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.o.weight\n",
            "base_model.model.decoder.block.3.layer.0.layer_norm.weight\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.base_layer.weight\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.k.base_layer.weight\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.k.lora_A.default.weight\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.k.lora_B.default.weight\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.base_layer.weight\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.o.weight\n",
            "base_model.model.decoder.block.3.layer.1.layer_norm.weight\n",
            "base_model.model.decoder.block.3.layer.2.DenseReluDense.wi_0.weight\n",
            "base_model.model.decoder.block.3.layer.2.DenseReluDense.wi_1.weight\n",
            "base_model.model.decoder.block.3.layer.2.DenseReluDense.wo.base_layer.weight\n",
            "base_model.model.decoder.block.3.layer.2.DenseReluDense.wo.lora_A.default.weight\n",
            "base_model.model.decoder.block.3.layer.2.DenseReluDense.wo.lora_B.default.weight\n",
            "base_model.model.decoder.block.3.layer.2.layer_norm.weight\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.q.base_layer.weight\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.k.base_layer.weight\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.k.lora_A.default.weight\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.k.lora_B.default.weight\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.v.base_layer.weight\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.o.weight\n",
            "base_model.model.decoder.block.4.layer.0.layer_norm.weight\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.base_layer.weight\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.k.base_layer.weight\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.k.lora_A.default.weight\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.k.lora_B.default.weight\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.base_layer.weight\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.o.weight\n",
            "base_model.model.decoder.block.4.layer.1.layer_norm.weight\n",
            "base_model.model.decoder.block.4.layer.2.DenseReluDense.wi_0.weight\n",
            "base_model.model.decoder.block.4.layer.2.DenseReluDense.wi_1.weight\n",
            "base_model.model.decoder.block.4.layer.2.DenseReluDense.wo.base_layer.weight\n",
            "base_model.model.decoder.block.4.layer.2.DenseReluDense.wo.lora_A.default.weight\n",
            "base_model.model.decoder.block.4.layer.2.DenseReluDense.wo.lora_B.default.weight\n",
            "base_model.model.decoder.block.4.layer.2.layer_norm.weight\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.q.base_layer.weight\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.k.base_layer.weight\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.k.lora_A.default.weight\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.k.lora_B.default.weight\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.v.base_layer.weight\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.o.weight\n",
            "base_model.model.decoder.block.5.layer.0.layer_norm.weight\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.base_layer.weight\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.k.base_layer.weight\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.k.lora_A.default.weight\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.k.lora_B.default.weight\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.base_layer.weight\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.o.weight\n",
            "base_model.model.decoder.block.5.layer.1.layer_norm.weight\n",
            "base_model.model.decoder.block.5.layer.2.DenseReluDense.wi_0.weight\n",
            "base_model.model.decoder.block.5.layer.2.DenseReluDense.wi_1.weight\n",
            "base_model.model.decoder.block.5.layer.2.DenseReluDense.wo.base_layer.weight\n",
            "base_model.model.decoder.block.5.layer.2.DenseReluDense.wo.lora_A.default.weight\n",
            "base_model.model.decoder.block.5.layer.2.DenseReluDense.wo.lora_B.default.weight\n",
            "base_model.model.decoder.block.5.layer.2.layer_norm.weight\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.q.base_layer.weight\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.k.base_layer.weight\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.k.lora_A.default.weight\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.k.lora_B.default.weight\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.v.base_layer.weight\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.o.weight\n",
            "base_model.model.decoder.block.6.layer.0.layer_norm.weight\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.base_layer.weight\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.k.base_layer.weight\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.k.lora_A.default.weight\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.k.lora_B.default.weight\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.base_layer.weight\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.o.weight\n",
            "base_model.model.decoder.block.6.layer.1.layer_norm.weight\n",
            "base_model.model.decoder.block.6.layer.2.DenseReluDense.wi_0.weight\n",
            "base_model.model.decoder.block.6.layer.2.DenseReluDense.wi_1.weight\n",
            "base_model.model.decoder.block.6.layer.2.DenseReluDense.wo.base_layer.weight\n",
            "base_model.model.decoder.block.6.layer.2.DenseReluDense.wo.lora_A.default.weight\n",
            "base_model.model.decoder.block.6.layer.2.DenseReluDense.wo.lora_B.default.weight\n",
            "base_model.model.decoder.block.6.layer.2.layer_norm.weight\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.q.base_layer.weight\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.k.base_layer.weight\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.k.lora_A.default.weight\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.k.lora_B.default.weight\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.v.base_layer.weight\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.o.weight\n",
            "base_model.model.decoder.block.7.layer.0.layer_norm.weight\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.base_layer.weight\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_A.default.weight\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_B.default.weight\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.k.base_layer.weight\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.k.lora_A.default.weight\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.k.lora_B.default.weight\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.base_layer.weight\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_A.default.weight\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_B.default.weight\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.o.weight\n",
            "base_model.model.decoder.block.7.layer.1.layer_norm.weight\n",
            "base_model.model.decoder.block.7.layer.2.DenseReluDense.wi_0.weight\n",
            "base_model.model.decoder.block.7.layer.2.DenseReluDense.wi_1.weight\n",
            "base_model.model.decoder.block.7.layer.2.DenseReluDense.wo.base_layer.weight\n",
            "base_model.model.decoder.block.7.layer.2.DenseReluDense.wo.lora_A.default.weight\n",
            "base_model.model.decoder.block.7.layer.2.DenseReluDense.wo.lora_B.default.weight\n",
            "base_model.model.decoder.block.7.layer.2.layer_norm.weight\n",
            "base_model.model.decoder.final_layer_norm.weight\n",
            "base_model.model.lm_head.base_layer.weight\n",
            "base_model.model.lm_head.lora_A.default.weight\n",
            "base_model.model.lm_head.lora_B.default.weight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# put the name of the file below in the load"
      ],
      "metadata": {
        "id": "5ZDKie5lnaQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(state_dict, strict=False)"
      ],
      "metadata": {
        "id": "fY_3ogauoiyg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89950362-8b0c-42cd-bec5-3037ab9e03e3"
      },
      "execution_count": 430,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['base_model.model.shared.weight', 'base_model.model.encoder.embed_tokens.weight', 'base_model.model.encoder.block.0.layer.0.SelfAttention.q.base_layer.weight', 'base_model.model.encoder.block.0.layer.0.SelfAttention.k.base_layer.weight', 'base_model.model.encoder.block.0.layer.0.SelfAttention.k.lora_A.default.weight', 'base_model.model.encoder.block.0.layer.0.SelfAttention.k.lora_B.default.weight', 'base_model.model.encoder.block.0.layer.0.SelfAttention.v.base_layer.weight', 'base_model.model.encoder.block.0.layer.0.SelfAttention.o.weight', 'base_model.model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'base_model.model.encoder.block.0.layer.0.layer_norm.weight', 'base_model.model.encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 'base_model.model.encoder.block.0.layer.1.DenseReluDense.wi_1.weight', 'base_model.model.encoder.block.0.layer.1.DenseReluDense.wo.base_layer.weight', 'base_model.model.encoder.block.0.layer.1.DenseReluDense.wo.lora_A.default.weight', 'base_model.model.encoder.block.0.layer.1.DenseReluDense.wo.lora_B.default.weight', 'base_model.model.encoder.block.0.layer.1.layer_norm.weight', 'base_model.model.encoder.block.1.layer.0.SelfAttention.q.base_layer.weight', 'base_model.model.encoder.block.1.layer.0.SelfAttention.k.base_layer.weight', 'base_model.model.encoder.block.1.layer.0.SelfAttention.k.lora_A.default.weight', 'base_model.model.encoder.block.1.layer.0.SelfAttention.k.lora_B.default.weight', 'base_model.model.encoder.block.1.layer.0.SelfAttention.v.base_layer.weight', 'base_model.model.encoder.block.1.layer.0.SelfAttention.o.weight', 'base_model.model.encoder.block.1.layer.0.layer_norm.weight', 'base_model.model.encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 'base_model.model.encoder.block.1.layer.1.DenseReluDense.wi_1.weight', 'base_model.model.encoder.block.1.layer.1.DenseReluDense.wo.base_layer.weight', 'base_model.model.encoder.block.1.layer.1.DenseReluDense.wo.lora_A.default.weight', 'base_model.model.encoder.block.1.layer.1.DenseReluDense.wo.lora_B.default.weight', 'base_model.model.encoder.block.1.layer.1.layer_norm.weight', 'base_model.model.encoder.block.2.layer.0.SelfAttention.q.base_layer.weight', 'base_model.model.encoder.block.2.layer.0.SelfAttention.k.base_layer.weight', 'base_model.model.encoder.block.2.layer.0.SelfAttention.k.lora_A.default.weight', 'base_model.model.encoder.block.2.layer.0.SelfAttention.k.lora_B.default.weight', 'base_model.model.encoder.block.2.layer.0.SelfAttention.v.base_layer.weight', 'base_model.model.encoder.block.2.layer.0.SelfAttention.o.weight', 'base_model.model.encoder.block.2.layer.0.layer_norm.weight', 'base_model.model.encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'base_model.model.encoder.block.2.layer.1.DenseReluDense.wi_1.weight', 'base_model.model.encoder.block.2.layer.1.DenseReluDense.wo.base_layer.weight', 'base_model.model.encoder.block.2.layer.1.DenseReluDense.wo.lora_A.default.weight', 'base_model.model.encoder.block.2.layer.1.DenseReluDense.wo.lora_B.default.weight', 'base_model.model.encoder.block.2.layer.1.layer_norm.weight', 'base_model.model.encoder.block.3.layer.0.SelfAttention.q.base_layer.weight', 'base_model.model.encoder.block.3.layer.0.SelfAttention.k.base_layer.weight', 'base_model.model.encoder.block.3.layer.0.SelfAttention.k.lora_A.default.weight', 'base_model.model.encoder.block.3.layer.0.SelfAttention.k.lora_B.default.weight', 'base_model.model.encoder.block.3.layer.0.SelfAttention.v.base_layer.weight', 'base_model.model.encoder.block.3.layer.0.SelfAttention.o.weight', 'base_model.model.encoder.block.3.layer.0.layer_norm.weight', 'base_model.model.encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'base_model.model.encoder.block.3.layer.1.DenseReluDense.wi_1.weight', 'base_model.model.encoder.block.3.layer.1.DenseReluDense.wo.base_layer.weight', 'base_model.model.encoder.block.3.layer.1.DenseReluDense.wo.lora_A.default.weight', 'base_model.model.encoder.block.3.layer.1.DenseReluDense.wo.lora_B.default.weight', 'base_model.model.encoder.block.3.layer.1.layer_norm.weight', 'base_model.model.encoder.block.4.layer.0.SelfAttention.q.base_layer.weight', 'base_model.model.encoder.block.4.layer.0.SelfAttention.k.base_layer.weight', 'base_model.model.encoder.block.4.layer.0.SelfAttention.k.lora_A.default.weight', 'base_model.model.encoder.block.4.layer.0.SelfAttention.k.lora_B.default.weight', 'base_model.model.encoder.block.4.layer.0.SelfAttention.v.base_layer.weight', 'base_model.model.encoder.block.4.layer.0.SelfAttention.o.weight', 'base_model.model.encoder.block.4.layer.0.layer_norm.weight', 'base_model.model.encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'base_model.model.encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'base_model.model.encoder.block.4.layer.1.DenseReluDense.wo.base_layer.weight', 'base_model.model.encoder.block.4.layer.1.DenseReluDense.wo.lora_A.default.weight', 'base_model.model.encoder.block.4.layer.1.DenseReluDense.wo.lora_B.default.weight', 'base_model.model.encoder.block.4.layer.1.layer_norm.weight', 'base_model.model.encoder.block.5.layer.0.SelfAttention.q.base_layer.weight', 'base_model.model.encoder.block.5.layer.0.SelfAttention.k.base_layer.weight', 'base_model.model.encoder.block.5.layer.0.SelfAttention.k.lora_A.default.weight', 'base_model.model.encoder.block.5.layer.0.SelfAttention.k.lora_B.default.weight', 'base_model.model.encoder.block.5.layer.0.SelfAttention.v.base_layer.weight', 'base_model.model.encoder.block.5.layer.0.SelfAttention.o.weight', 'base_model.model.encoder.block.5.layer.0.layer_norm.weight', 'base_model.model.encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'base_model.model.encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 'base_model.model.encoder.block.5.layer.1.DenseReluDense.wo.base_layer.weight', 'base_model.model.encoder.block.5.layer.1.DenseReluDense.wo.lora_A.default.weight', 'base_model.model.encoder.block.5.layer.1.DenseReluDense.wo.lora_B.default.weight', 'base_model.model.encoder.block.5.layer.1.layer_norm.weight', 'base_model.model.encoder.block.6.layer.0.SelfAttention.q.base_layer.weight', 'base_model.model.encoder.block.6.layer.0.SelfAttention.k.base_layer.weight', 'base_model.model.encoder.block.6.layer.0.SelfAttention.k.lora_A.default.weight', 'base_model.model.encoder.block.6.layer.0.SelfAttention.k.lora_B.default.weight', 'base_model.model.encoder.block.6.layer.0.SelfAttention.v.base_layer.weight', 'base_model.model.encoder.block.6.layer.0.SelfAttention.o.weight', 'base_model.model.encoder.block.6.layer.0.layer_norm.weight', 'base_model.model.encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'base_model.model.encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'base_model.model.encoder.block.6.layer.1.DenseReluDense.wo.base_layer.weight', 'base_model.model.encoder.block.6.layer.1.DenseReluDense.wo.lora_A.default.weight', 'base_model.model.encoder.block.6.layer.1.DenseReluDense.wo.lora_B.default.weight', 'base_model.model.encoder.block.6.layer.1.layer_norm.weight', 'base_model.model.encoder.block.7.layer.0.SelfAttention.q.base_layer.weight', 'base_model.model.encoder.block.7.layer.0.SelfAttention.k.base_layer.weight', 'base_model.model.encoder.block.7.layer.0.SelfAttention.k.lora_A.default.weight', 'base_model.model.encoder.block.7.layer.0.SelfAttention.k.lora_B.default.weight', 'base_model.model.encoder.block.7.layer.0.SelfAttention.v.base_layer.weight', 'base_model.model.encoder.block.7.layer.0.SelfAttention.o.weight', 'base_model.model.encoder.block.7.layer.0.layer_norm.weight', 'base_model.model.encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'base_model.model.encoder.block.7.layer.1.DenseReluDense.wi_1.weight', 'base_model.model.encoder.block.7.layer.1.DenseReluDense.wo.base_layer.weight', 'base_model.model.encoder.block.7.layer.1.DenseReluDense.wo.lora_A.default.weight', 'base_model.model.encoder.block.7.layer.1.DenseReluDense.wo.lora_B.default.weight', 'base_model.model.encoder.block.7.layer.1.layer_norm.weight', 'base_model.model.encoder.final_layer_norm.weight', 'base_model.model.decoder.embed_tokens.weight', 'base_model.model.decoder.block.0.layer.0.SelfAttention.q.base_layer.weight', 'base_model.model.decoder.block.0.layer.0.SelfAttention.k.base_layer.weight', 'base_model.model.decoder.block.0.layer.0.SelfAttention.k.lora_A.default.weight', 'base_model.model.decoder.block.0.layer.0.SelfAttention.k.lora_B.default.weight', 'base_model.model.decoder.block.0.layer.0.SelfAttention.v.base_layer.weight', 'base_model.model.decoder.block.0.layer.0.SelfAttention.o.weight', 'base_model.model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'base_model.model.decoder.block.0.layer.0.layer_norm.weight', 'base_model.model.decoder.block.0.layer.1.EncDecAttention.q.base_layer.weight', 'base_model.model.decoder.block.0.layer.1.EncDecAttention.k.base_layer.weight', 'base_model.model.decoder.block.0.layer.1.EncDecAttention.k.lora_A.default.weight', 'base_model.model.decoder.block.0.layer.1.EncDecAttention.k.lora_B.default.weight', 'base_model.model.decoder.block.0.layer.1.EncDecAttention.v.base_layer.weight', 'base_model.model.decoder.block.0.layer.1.EncDecAttention.o.weight', 'base_model.model.decoder.block.0.layer.1.layer_norm.weight', 'base_model.model.decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'base_model.model.decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'base_model.model.decoder.block.0.layer.2.DenseReluDense.wo.base_layer.weight', 'base_model.model.decoder.block.0.layer.2.DenseReluDense.wo.lora_A.default.weight', 'base_model.model.decoder.block.0.layer.2.DenseReluDense.wo.lora_B.default.weight', 'base_model.model.decoder.block.0.layer.2.layer_norm.weight', 'base_model.model.decoder.block.1.layer.0.SelfAttention.q.base_layer.weight', 'base_model.model.decoder.block.1.layer.0.SelfAttention.k.base_layer.weight', 'base_model.model.decoder.block.1.layer.0.SelfAttention.k.lora_A.default.weight', 'base_model.model.decoder.block.1.layer.0.SelfAttention.k.lora_B.default.weight', 'base_model.model.decoder.block.1.layer.0.SelfAttention.v.base_layer.weight', 'base_model.model.decoder.block.1.layer.0.SelfAttention.o.weight', 'base_model.model.decoder.block.1.layer.0.layer_norm.weight', 'base_model.model.decoder.block.1.layer.1.EncDecAttention.q.base_layer.weight', 'base_model.model.decoder.block.1.layer.1.EncDecAttention.k.base_layer.weight', 'base_model.model.decoder.block.1.layer.1.EncDecAttention.k.lora_A.default.weight', 'base_model.model.decoder.block.1.layer.1.EncDecAttention.k.lora_B.default.weight', 'base_model.model.decoder.block.1.layer.1.EncDecAttention.v.base_layer.weight', 'base_model.model.decoder.block.1.layer.1.EncDecAttention.o.weight', 'base_model.model.decoder.block.1.layer.1.layer_norm.weight', 'base_model.model.decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'base_model.model.decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'base_model.model.decoder.block.1.layer.2.DenseReluDense.wo.base_layer.weight', 'base_model.model.decoder.block.1.layer.2.DenseReluDense.wo.lora_A.default.weight', 'base_model.model.decoder.block.1.layer.2.DenseReluDense.wo.lora_B.default.weight', 'base_model.model.decoder.block.1.layer.2.layer_norm.weight', 'base_model.model.decoder.block.2.layer.0.SelfAttention.q.base_layer.weight', 'base_model.model.decoder.block.2.layer.0.SelfAttention.k.base_layer.weight', 'base_model.model.decoder.block.2.layer.0.SelfAttention.k.lora_A.default.weight', 'base_model.model.decoder.block.2.layer.0.SelfAttention.k.lora_B.default.weight', 'base_model.model.decoder.block.2.layer.0.SelfAttention.v.base_layer.weight', 'base_model.model.decoder.block.2.layer.0.SelfAttention.o.weight', 'base_model.model.decoder.block.2.layer.0.layer_norm.weight', 'base_model.model.decoder.block.2.layer.1.EncDecAttention.q.base_layer.weight', 'base_model.model.decoder.block.2.layer.1.EncDecAttention.k.base_layer.weight', 'base_model.model.decoder.block.2.layer.1.EncDecAttention.k.lora_A.default.weight', 'base_model.model.decoder.block.2.layer.1.EncDecAttention.k.lora_B.default.weight', 'base_model.model.decoder.block.2.layer.1.EncDecAttention.v.base_layer.weight', 'base_model.model.decoder.block.2.layer.1.EncDecAttention.o.weight', 'base_model.model.decoder.block.2.layer.1.layer_norm.weight', 'base_model.model.decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'base_model.model.decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'base_model.model.decoder.block.2.layer.2.DenseReluDense.wo.base_layer.weight', 'base_model.model.decoder.block.2.layer.2.DenseReluDense.wo.lora_A.default.weight', 'base_model.model.decoder.block.2.layer.2.DenseReluDense.wo.lora_B.default.weight', 'base_model.model.decoder.block.2.layer.2.layer_norm.weight', 'base_model.model.decoder.block.3.layer.0.SelfAttention.q.base_layer.weight', 'base_model.model.decoder.block.3.layer.0.SelfAttention.k.base_layer.weight', 'base_model.model.decoder.block.3.layer.0.SelfAttention.k.lora_A.default.weight', 'base_model.model.decoder.block.3.layer.0.SelfAttention.k.lora_B.default.weight', 'base_model.model.decoder.block.3.layer.0.SelfAttention.v.base_layer.weight', 'base_model.model.decoder.block.3.layer.0.SelfAttention.o.weight', 'base_model.model.decoder.block.3.layer.0.layer_norm.weight', 'base_model.model.decoder.block.3.layer.1.EncDecAttention.q.base_layer.weight', 'base_model.model.decoder.block.3.layer.1.EncDecAttention.k.base_layer.weight', 'base_model.model.decoder.block.3.layer.1.EncDecAttention.k.lora_A.default.weight', 'base_model.model.decoder.block.3.layer.1.EncDecAttention.k.lora_B.default.weight', 'base_model.model.decoder.block.3.layer.1.EncDecAttention.v.base_layer.weight', 'base_model.model.decoder.block.3.layer.1.EncDecAttention.o.weight', 'base_model.model.decoder.block.3.layer.1.layer_norm.weight', 'base_model.model.decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'base_model.model.decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'base_model.model.decoder.block.3.layer.2.DenseReluDense.wo.base_layer.weight', 'base_model.model.decoder.block.3.layer.2.DenseReluDense.wo.lora_A.default.weight', 'base_model.model.decoder.block.3.layer.2.DenseReluDense.wo.lora_B.default.weight', 'base_model.model.decoder.block.3.layer.2.layer_norm.weight', 'base_model.model.decoder.block.4.layer.0.SelfAttention.q.base_layer.weight', 'base_model.model.decoder.block.4.layer.0.SelfAttention.k.base_layer.weight', 'base_model.model.decoder.block.4.layer.0.SelfAttention.k.lora_A.default.weight', 'base_model.model.decoder.block.4.layer.0.SelfAttention.k.lora_B.default.weight', 'base_model.model.decoder.block.4.layer.0.SelfAttention.v.base_layer.weight', 'base_model.model.decoder.block.4.layer.0.SelfAttention.o.weight', 'base_model.model.decoder.block.4.layer.0.layer_norm.weight', 'base_model.model.decoder.block.4.layer.1.EncDecAttention.q.base_layer.weight', 'base_model.model.decoder.block.4.layer.1.EncDecAttention.k.base_layer.weight', 'base_model.model.decoder.block.4.layer.1.EncDecAttention.k.lora_A.default.weight', 'base_model.model.decoder.block.4.layer.1.EncDecAttention.k.lora_B.default.weight', 'base_model.model.decoder.block.4.layer.1.EncDecAttention.v.base_layer.weight', 'base_model.model.decoder.block.4.layer.1.EncDecAttention.o.weight', 'base_model.model.decoder.block.4.layer.1.layer_norm.weight', 'base_model.model.decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'base_model.model.decoder.block.4.layer.2.DenseReluDense.wi_1.weight', 'base_model.model.decoder.block.4.layer.2.DenseReluDense.wo.base_layer.weight', 'base_model.model.decoder.block.4.layer.2.DenseReluDense.wo.lora_A.default.weight', 'base_model.model.decoder.block.4.layer.2.DenseReluDense.wo.lora_B.default.weight', 'base_model.model.decoder.block.4.layer.2.layer_norm.weight', 'base_model.model.decoder.block.5.layer.0.SelfAttention.q.base_layer.weight', 'base_model.model.decoder.block.5.layer.0.SelfAttention.k.base_layer.weight', 'base_model.model.decoder.block.5.layer.0.SelfAttention.k.lora_A.default.weight', 'base_model.model.decoder.block.5.layer.0.SelfAttention.k.lora_B.default.weight', 'base_model.model.decoder.block.5.layer.0.SelfAttention.v.base_layer.weight', 'base_model.model.decoder.block.5.layer.0.SelfAttention.o.weight', 'base_model.model.decoder.block.5.layer.0.layer_norm.weight', 'base_model.model.decoder.block.5.layer.1.EncDecAttention.q.base_layer.weight', 'base_model.model.decoder.block.5.layer.1.EncDecAttention.k.base_layer.weight', 'base_model.model.decoder.block.5.layer.1.EncDecAttention.k.lora_A.default.weight', 'base_model.model.decoder.block.5.layer.1.EncDecAttention.k.lora_B.default.weight', 'base_model.model.decoder.block.5.layer.1.EncDecAttention.v.base_layer.weight', 'base_model.model.decoder.block.5.layer.1.EncDecAttention.o.weight', 'base_model.model.decoder.block.5.layer.1.layer_norm.weight', 'base_model.model.decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'base_model.model.decoder.block.5.layer.2.DenseReluDense.wi_1.weight', 'base_model.model.decoder.block.5.layer.2.DenseReluDense.wo.base_layer.weight', 'base_model.model.decoder.block.5.layer.2.DenseReluDense.wo.lora_A.default.weight', 'base_model.model.decoder.block.5.layer.2.DenseReluDense.wo.lora_B.default.weight', 'base_model.model.decoder.block.5.layer.2.layer_norm.weight', 'base_model.model.decoder.block.6.layer.0.SelfAttention.q.base_layer.weight', 'base_model.model.decoder.block.6.layer.0.SelfAttention.k.base_layer.weight', 'base_model.model.decoder.block.6.layer.0.SelfAttention.k.lora_A.default.weight', 'base_model.model.decoder.block.6.layer.0.SelfAttention.k.lora_B.default.weight', 'base_model.model.decoder.block.6.layer.0.SelfAttention.v.base_layer.weight', 'base_model.model.decoder.block.6.layer.0.SelfAttention.o.weight', 'base_model.model.decoder.block.6.layer.0.layer_norm.weight', 'base_model.model.decoder.block.6.layer.1.EncDecAttention.q.base_layer.weight', 'base_model.model.decoder.block.6.layer.1.EncDecAttention.k.base_layer.weight', 'base_model.model.decoder.block.6.layer.1.EncDecAttention.k.lora_A.default.weight', 'base_model.model.decoder.block.6.layer.1.EncDecAttention.k.lora_B.default.weight', 'base_model.model.decoder.block.6.layer.1.EncDecAttention.v.base_layer.weight', 'base_model.model.decoder.block.6.layer.1.EncDecAttention.o.weight', 'base_model.model.decoder.block.6.layer.1.layer_norm.weight', 'base_model.model.decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'base_model.model.decoder.block.6.layer.2.DenseReluDense.wi_1.weight', 'base_model.model.decoder.block.6.layer.2.DenseReluDense.wo.base_layer.weight', 'base_model.model.decoder.block.6.layer.2.DenseReluDense.wo.lora_A.default.weight', 'base_model.model.decoder.block.6.layer.2.DenseReluDense.wo.lora_B.default.weight', 'base_model.model.decoder.block.6.layer.2.layer_norm.weight', 'base_model.model.decoder.block.7.layer.0.SelfAttention.q.base_layer.weight', 'base_model.model.decoder.block.7.layer.0.SelfAttention.k.base_layer.weight', 'base_model.model.decoder.block.7.layer.0.SelfAttention.k.lora_A.default.weight', 'base_model.model.decoder.block.7.layer.0.SelfAttention.k.lora_B.default.weight', 'base_model.model.decoder.block.7.layer.0.SelfAttention.v.base_layer.weight', 'base_model.model.decoder.block.7.layer.0.SelfAttention.o.weight', 'base_model.model.decoder.block.7.layer.0.layer_norm.weight', 'base_model.model.decoder.block.7.layer.1.EncDecAttention.q.base_layer.weight', 'base_model.model.decoder.block.7.layer.1.EncDecAttention.k.base_layer.weight', 'base_model.model.decoder.block.7.layer.1.EncDecAttention.k.lora_A.default.weight', 'base_model.model.decoder.block.7.layer.1.EncDecAttention.k.lora_B.default.weight', 'base_model.model.decoder.block.7.layer.1.EncDecAttention.v.base_layer.weight', 'base_model.model.decoder.block.7.layer.1.EncDecAttention.o.weight', 'base_model.model.decoder.block.7.layer.1.layer_norm.weight', 'base_model.model.decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'base_model.model.decoder.block.7.layer.2.DenseReluDense.wi_1.weight', 'base_model.model.decoder.block.7.layer.2.DenseReluDense.wo.base_layer.weight', 'base_model.model.decoder.block.7.layer.2.DenseReluDense.wo.lora_A.default.weight', 'base_model.model.decoder.block.7.layer.2.DenseReluDense.wo.lora_B.default.weight', 'base_model.model.decoder.block.7.layer.2.layer_norm.weight', 'base_model.model.decoder.final_layer_norm.weight', 'base_model.model.lm_head.base_layer.weight', 'base_model.model.lm_head.lora_A.default.weight', 'base_model.model.lm_head.lora_B.default.weight'], unexpected_keys=['_module_functions.value.value_head_op.0.weight', '_module_functions.value.value_head_op.0.bias', '_module_functions.value.value_head_op.2.weight', '_module_functions.value.value_head_op.2.bias', '_module_functions.value.value_head_op.4.weight', '_module_functions.value.value_head_op.4.bias'])"
            ]
          },
          "metadata": {},
          "execution_count": 430
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install inseq"
      ],
      "metadata": {
        "id": "pPcYwMM1AFM_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31698ce9-aff5-45d7-add0-9761127c6e97"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: inseq in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: rich>=10.13.0 in /usr/local/lib/python3.11/dist-packages (from inseq) (13.9.4)\n",
            "Requirement already satisfied: transformers>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece,tokenizers]>=4.22.0->inseq) (4.49.0)\n",
            "Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.11/dist-packages (from inseq) (5.29.4)\n",
            "Requirement already satisfied: captum>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from inseq) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from inseq) (2.0.2)\n",
            "Requirement already satisfied: jaxtyping>=0.2.25 in /usr/local/lib/python3.11/dist-packages (from inseq) (0.3.0)\n",
            "Requirement already satisfied: typeguard<=2.13.3 in /usr/local/lib/python3.11/dist-packages (from inseq) (2.13.3)\n",
            "Requirement already satisfied: torch>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from inseq) (2.6.0+cu124)\n",
            "Requirement already satisfied: matplotlib>=3.5.3 in /usr/local/lib/python3.11/dist-packages (from inseq) (3.10.0)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from inseq) (4.67.1)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from jaxtyping>=0.2.25->inseq) (0.1.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->inseq) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->inseq) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->inseq) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->inseq) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->inseq) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->inseq) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->inseq) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->inseq) (2.8.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.13.0->inseq) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.13.0->inseq) (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->inseq) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.1->inseq) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.22.0->transformers[sentencepiece,tokenizers]>=4.22.0->inseq) (0.29.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.22.0->transformers[sentencepiece,tokenizers]>=4.22.0->inseq) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.22.0->transformers[sentencepiece,tokenizers]>=4.22.0->inseq) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.22.0->transformers[sentencepiece,tokenizers]>=4.22.0->inseq) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.22.0->transformers[sentencepiece,tokenizers]>=4.22.0->inseq) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.22.0->transformers[sentencepiece,tokenizers]>=4.22.0->inseq) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece,tokenizers]>=4.22.0->inseq) (0.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.13.0->inseq) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.3->inseq) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.1->inseq) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.22.0->transformers[sentencepiece,tokenizers]>=4.22.0->inseq) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.22.0->transformers[sentencepiece,tokenizers]>=4.22.0->inseq) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.22.0->transformers[sentencepiece,tokenizers]>=4.22.0->inseq) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.22.0->transformers[sentencepiece,tokenizers]>=4.22.0->inseq) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.output_attentions = True\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20nxhM6AUa4u",
        "outputId": "176156d7-ab9e-470e-edd2-a15c0449ec50"
      },
      "execution_count": 431,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForSeq2SeqLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): T5ForConditionalGeneration(\n",
              "      (shared): Embedding(32128, 512)\n",
              "      (encoder): T5Stack(\n",
              "        (embed_tokens): Embedding(32128, 512)\n",
              "        (block): ModuleList(\n",
              "          (0): T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                  (relative_attention_bias): Embedding(32, 6)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wo): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1-7): 7 x T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wo): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_layer_norm): T5LayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (decoder): T5Stack(\n",
              "        (embed_tokens): Embedding(32128, 512)\n",
              "        (block): ModuleList(\n",
              "          (0): T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                  (relative_attention_bias): Embedding(32, 6)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerCrossAttention(\n",
              "                (EncDecAttention): T5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (2): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wo): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1-7): 7 x T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerCrossAttention(\n",
              "                (EncDecAttention): T5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (2): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wo): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_layer_norm): T5LayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (lm_head): lora.Linear(\n",
              "        (base_layer): Linear(in_features=512, out_features=32128, bias=False)\n",
              "        (lora_dropout): ModuleDict(\n",
              "          (default): Identity()\n",
              "        )\n",
              "        (lora_A): ModuleDict(\n",
              "          (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "        )\n",
              "        (lora_B): ModuleDict(\n",
              "          (default): Linear(in_features=32, out_features=32128, bias=False)\n",
              "        )\n",
              "        (lora_embedding_A): ParameterDict()\n",
              "        (lora_embedding_B): ParameterDict()\n",
              "        (lora_magnitude_vector): ModuleDict()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 431
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"<Begin.Possible actions>turn left, turn right, go forward, pick up, drop, toggle <End \\\n",
        " Possible actions> \\\n",
        " <Begin Goal> go to a grey box<End Goal>\\\n",
        " <Begin Current Observation>\\\n",
        " Observation: You see a wall 3 steps forward, You see a wall 2 steps\\\n",
        " left, You see a grey ball 1 step right and 1 step forward, You see a\\\n",
        " grey box 2 steps right and 1 step forward, You see a grey box 3 steps\\\n",
        " right and 1 step forward<End Current Observation>\\\n",
        " Next action :\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "z7STSusyU38j"
      },
      "execution_count": 432,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajout minimal d'un token au dcodeur\n",
        "decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        decoder_input_ids=decoder_input_ids,\n",
        "        output_attentions=True,\n",
        "        return_dict=True\n",
        "    )\n",
        "# Extract attention\n",
        "decoder_attentions = outputs.decoder_attentions  # (num_layers, batch, heads, tgt_len, tgt_len)\n",
        "encoder_attentions = outputs.encoder_attentions  # (num_layers, batch, heads, src_len, src_len)\n",
        "cross_attentions   = outputs.cross_attentions  # (batch_size, num_heads, seq_len, seq_len)\n"
      ],
      "metadata": {
        "id": "yn2KULeOU6A9"
      },
      "execution_count": 433,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "tgt_tokens = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])"
      ],
      "metadata": {
        "id": "jvxUecBKqkM4"
      },
      "execution_count": 434,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_attentions[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hal5jj1Yp4ya",
        "outputId": "adcb749c-52e8-4aef-e900-e634ea5f0bcf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 139, 139])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_attentions[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGmAtlxEqEQA",
        "outputId": "63b3cbb4-7018-4696-9611-4b0d1361d4bb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calcule the average attention in the final head\n",
        "att = cross_attentions[-1][0].mean(dim=0).detach().numpy()\n"
      ],
      "metadata": {
        "id": "kZ05G5weoCOU"
      },
      "execution_count": 435,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_1 = att[0][:35]\n",
        "tensor_2 = att[0][35:53]\n",
        "tensor_3 = att[0][53:133]"
      ],
      "metadata": {
        "id": "XJCS3i7xdDtc"
      },
      "execution_count": 404,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_1_mean = np.mean(tensor_1)\n",
        "tensor_2_mean = np.mean(tensor_2)\n",
        "tensor_3_mean = np.mean(tensor_3)\n",
        "\n",
        "sum_tensor = tensor_1_mean + tensor_2_mean + tensor_3_mean\n",
        "\n",
        "list_attention0 = np.stack([tensor_1_mean, tensor_2_mean, tensor_3_mean])/sum_tensor\n",
        "names_abscisse0 = ['Possible Actions', 'Goal', 'Observation']"
      ],
      "metadata": {
        "id": "V5tuRoEBeTpp"
      },
      "execution_count": 436,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(30, 6))\n",
        "sns.heatmap(list_attention0.reshape(1, -1), xticklabels=names_abscisse0, yticklabels=tgt_tokens, cmap=\"viridis\")\n",
        "plt.title(\"Cross-Attention Map (Last Layer, Heads)\")\n",
        "plt.xlabel(\"Source Tokens\")\n",
        "plt.ylabel(\"Decoder Tokens\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "-pGV84PFfIZQ",
        "outputId": "7b9b43a3-2ddc-4212-d291-aa4dafa51b02"
      },
      "execution_count": 437,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 3000x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatterpolar(\n",
        "    r=list_attention0,\n",
        "    theta=names_abscisse0,\n",
        "    fill='toself',\n",
        "    name='Attention',\n",
        "    line=dict(color='royalblue', width=2)\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(\n",
        "        text=\"Spider Graph of Attention\",\n",
        "        x=0.5,\n",
        "        xanchor='center',\n",
        "        font=dict(size=20, family=\"Arial\", color=\"black\")\n",
        "    ),\n",
        "    polar=dict(\n",
        "        bgcolor=\"rgba(0,0,0,0)\",\n",
        "        radialaxis=dict(\n",
        "            visible=True,\n",
        "            range=[0, 1],\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray',\n",
        "            gridwidth=1,\n",
        "            linewidth=2,\n",
        "            linecolor='gray'\n",
        "        ),\n",
        "        angularaxis=dict(\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray'\n",
        "        )\n",
        "    ),\n",
        "    showlegend=False,\n",
        "    paper_bgcolor=\"white\",\n",
        "    font=dict(family=\"Arial\", size=14, color=\"black\")\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "8gM1mbTmsUrc",
        "outputId": "54e07cc7-840f-4efa-8403-49fdeb68595e"
      },
      "execution_count": 438,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"c2c3c24a-f807-4133-a50c-f8db8a909d42\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c2c3c24a-f807-4133-a50c-f8db8a909d42\")) {                    Plotly.newPlot(                        \"c2c3c24a-f807-4133-a50c-f8db8a909d42\",                        [{\"fill\":\"toself\",\"line\":{\"color\":\"royalblue\",\"width\":2},\"name\":\"Attention\",\"r\":[0.19486354,0.23196857,0.57316786],\"theta\":[\"Possible Actions\",\"Goal\",\"Observation\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"font\":{\"size\":20,\"family\":\"Arial\",\"color\":\"black\"},\"text\":\"Spider Graph of Attention\",\"x\":0.5,\"xanchor\":\"center\"},\"polar\":{\"radialaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"visible\":true,\"range\":[0,1],\"gridcolor\":\"lightgray\",\"gridwidth\":1,\"linewidth\":2,\"linecolor\":\"gray\"},\"angularaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"gridcolor\":\"lightgray\"},\"bgcolor\":\"rgba(0,0,0,0)\"},\"font\":{\"family\":\"Arial\",\"size\":14,\"color\":\"black\"},\"showlegend\":false,\"paper_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c2c3c24a-f807-4133-a50c-f8db8a909d42');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Possible actions of the agent: turn left, turn right, go forward, pick up, drop,toggle \\\n",
        "Goal of the agent: go to the purple box \\\n",
        "Observation: You see a wall 2 steps forward, You see a purple box 2 steps left, You \\\n",
        "see a purple ball 1 step right and 1 step forward, You\\\n",
        "see a grey key 2 steps right Next action :\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Ajout minimal d'un token au dcodeur\n",
        "decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        decoder_input_ids=decoder_input_ids,\n",
        "        output_attentions=True,\n",
        "        return_dict=True\n",
        "    )\n",
        "# Extract attention\n",
        "decoder_attentions = outputs.decoder_attentions  # (num_layers, batch, heads, tgt_len, tgt_len)\n",
        "encoder_attentions = outputs.encoder_attentions  # (num_layers, batch, heads, src_len, src_len)\n",
        "cross_attentions   = outputs.cross_attentions  # (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "src_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "tgt_tokens = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n",
        "\n",
        "# calcule the average attention in the final head\n",
        "att = cross_attentions[-1][0].mean(dim=0).detach().numpy()\n",
        "\n",
        "tensor_1 = att[0][:23]\n",
        "tensor_2 = att[0][23:34]\n",
        "tensor_3 = att[0][34:-5]\n",
        "tensor_4 = att[0][-5:-1]\n",
        "\n",
        "tensor_1_mean = np.mean(tensor_1)\n",
        "tensor_2_mean = np.mean(tensor_2)\n",
        "tensor_3_mean = np.mean(tensor_3)\n",
        "\n",
        "sum_tensor = tensor_1_mean + tensor_2_mean + tensor_3_mean\n",
        "\n",
        "list_attention1 = np.stack([tensor_1_mean, tensor_2_mean, tensor_3_mean])/sum_tensor\n",
        "names_abscisse1 = ['Possible Actions', 'Goal', 'Observation']"
      ],
      "metadata": {
        "id": "f-x0lDcbt653"
      },
      "execution_count": 439,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatterpolar(\n",
        "    r=list_attention1,\n",
        "    theta=names_abscisse1,\n",
        "    fill='toself',\n",
        "    name='Attention',\n",
        "    line=dict(color='royalblue', width=2)\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(\n",
        "        text=\"Spider Graph of Attention\",\n",
        "        x=0.5,\n",
        "        xanchor='center',\n",
        "        font=dict(size=20, family=\"Arial\", color=\"black\")\n",
        "    ),\n",
        "    polar=dict(\n",
        "        bgcolor=\"rgba(0,0,0,0)\",\n",
        "        radialaxis=dict(\n",
        "            visible=True,\n",
        "            range=[0, 1],\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray',\n",
        "            gridwidth=1,\n",
        "            linewidth=2,\n",
        "            linecolor='gray'\n",
        "        ),\n",
        "        angularaxis=dict(\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray'\n",
        "        )\n",
        "    ),\n",
        "    showlegend=False,\n",
        "    paper_bgcolor=\"white\",\n",
        "    font=dict(family=\"Arial\", size=14, color=\"black\")\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "TrC0GP4XvJTk",
        "outputId": "ad66b7db-481f-413f-bb32-74c3bfa1bdc5"
      },
      "execution_count": 440,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"41ca9f6c-ddd5-4437-ac0e-1f696246d628\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"41ca9f6c-ddd5-4437-ac0e-1f696246d628\")) {                    Plotly.newPlot(                        \"41ca9f6c-ddd5-4437-ac0e-1f696246d628\",                        [{\"fill\":\"toself\",\"line\":{\"color\":\"royalblue\",\"width\":2},\"name\":\"Attention\",\"r\":[0.21381216,0.24743974,0.5387481],\"theta\":[\"Possible Actions\",\"Goal\",\"Observation\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"font\":{\"size\":20,\"family\":\"Arial\",\"color\":\"black\"},\"text\":\"Spider Graph of Attention\",\"x\":0.5,\"xanchor\":\"center\"},\"polar\":{\"radialaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"visible\":true,\"range\":[0,1],\"gridcolor\":\"lightgray\",\"gridwidth\":1,\"linewidth\":2,\"linecolor\":\"gray\"},\"angularaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"gridcolor\":\"lightgray\"},\"bgcolor\":\"rgba(0,0,0,0)\"},\"font\":{\"family\":\"Arial\",\"size\":14,\"color\":\"black\"},\"showlegend\":false,\"paper_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('41ca9f6c-ddd5-4437-ac0e-1f696246d628');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Goal of the agent: go to the purple box \\\n",
        " Possible actions of the agent: turn left, turn right, go forward, pick up, drop, toggle \\\n",
        " Observation: You see a wall 2 steps forward, You see a purple box 2 steps left, You \\\n",
        " see a purple ball 1 step right and 1 step forward, You \\\n",
        " see a grey key 2 steps right Next action :\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Ajout minimal d'un token au dcodeur\n",
        "decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        decoder_input_ids=decoder_input_ids,\n",
        "        output_attentions=True,\n",
        "        return_dict=True\n",
        "    )\n",
        "# Extract attention\n",
        "decoder_attentions = outputs.decoder_attentions  # (num_layers, batch, heads, tgt_len, tgt_len)\n",
        "encoder_attentions = outputs.encoder_attentions  # (num_layers, batch, heads, src_len, src_len)\n",
        "cross_attentions   = outputs.cross_attentions  # (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "src_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "tgt_tokens = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n",
        "\n",
        "# calcule the average attention in the final head\n",
        "att = cross_attentions[-1][0].mean(dim=0).detach().numpy()\n",
        "\n",
        "tensor_1 = att[0][0:10]\n",
        "tensor_2 = att[0][10:32]\n",
        "tensor_3 = att[0][32:-5]\n",
        "tensor_4 = att[0][-5:-1]\n",
        "\n",
        "tensor_1_mean = np.mean(tensor_1)\n",
        "tensor_2_mean = np.mean(tensor_2)\n",
        "tensor_3_mean = np.mean(tensor_3)\n",
        "tensor_4_mean = np.mean(tensor_4)\n",
        "\n",
        "sum_tensor = tensor_1_mean + tensor_2_mean + tensor_3_mean\n",
        "\n",
        "list_attention2 = np.stack([tensor_1_mean, tensor_2_mean, tensor_3_mean])/sum_tensor\n",
        "names_abscisse2 = ['Goal','Possible Actions','Observation']"
      ],
      "metadata": {
        "id": "FBvedqc7vW5p"
      },
      "execution_count": 441,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatterpolar(\n",
        "    r=list_attention2,\n",
        "    theta=names_abscisse2,\n",
        "    fill='toself',\n",
        "    name='Attention',\n",
        "    line=dict(color='royalblue', width=2)\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(\n",
        "        text=\"Spider Graph of Attention\",\n",
        "        x=0.5,\n",
        "        xanchor='center',\n",
        "        font=dict(size=20, family=\"Arial\", color=\"black\")\n",
        "    ),\n",
        "    polar=dict(\n",
        "        bgcolor=\"rgba(0,0,0,0)\",\n",
        "        radialaxis=dict(\n",
        "            visible=True,\n",
        "            range=[0, 1],\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray',\n",
        "            gridwidth=1,\n",
        "            linewidth=2,\n",
        "            linecolor='gray'\n",
        "        ),\n",
        "        angularaxis=dict(\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray'\n",
        "        )\n",
        "    ),\n",
        "    showlegend=False,\n",
        "    paper_bgcolor=\"white\",\n",
        "    font=dict(family=\"Arial\", size=14, color=\"black\")\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "BA_CIw_PvlOL",
        "outputId": "9bfae55e-b219-4d45-ab29-d8e5fa00561c"
      },
      "execution_count": 442,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"fd6f4577-7a08-4f34-89f7-0ed94f7f47e6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"fd6f4577-7a08-4f34-89f7-0ed94f7f47e6\")) {                    Plotly.newPlot(                        \"fd6f4577-7a08-4f34-89f7-0ed94f7f47e6\",                        [{\"fill\":\"toself\",\"line\":{\"color\":\"royalblue\",\"width\":2},\"name\":\"Attention\",\"r\":[0.2968205,0.3095703,0.39360914],\"theta\":[\"Goal\",\"Possible Actions\",\"Observation\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"font\":{\"size\":20,\"family\":\"Arial\",\"color\":\"black\"},\"text\":\"Spider Graph of Attention\",\"x\":0.5,\"xanchor\":\"center\"},\"polar\":{\"radialaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"visible\":true,\"range\":[0,1],\"gridcolor\":\"lightgray\",\"gridwidth\":1,\"linewidth\":2,\"linecolor\":\"gray\"},\"angularaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"gridcolor\":\"lightgray\"},\"bgcolor\":\"rgba(0,0,0,0)\"},\"font\":{\"family\":\"Arial\",\"size\":14,\"color\":\"black\"},\"showlegend\":false,\"paper_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('fd6f4577-7a08-4f34-89f7-0ed94f7f47e6');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"you are on a maze and you have to solve a task, what you can do is: turn left, turn \\\n",
        " right, go forward, pick up, drop, toggle your task is to go to a grey box, what you see \\\n",
        " now: You see a wall 3 steps forward, You see a wall 2 steps \\\n",
        " left, You see a grey ball 1 step right and 1 step forward, You see a grey box 2 steps\\\n",
        " right and 1 step forward, You see a grey box 3 steps right and 1 step forward\\\n",
        " and your next action is to\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Ajout minimal d'un token au dcodeur\n",
        "decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        decoder_input_ids=decoder_input_ids,\n",
        "        output_attentions=True,\n",
        "        return_dict=True\n",
        "    )\n",
        "# Extract attention\n",
        "decoder_attentions = outputs.decoder_attentions  # (num_layers, batch, heads, tgt_len, tgt_len)\n",
        "encoder_attentions = outputs.encoder_attentions  # (num_layers, batch, heads, src_len, src_len)\n",
        "cross_attentions   = outputs.cross_attentions  # (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "src_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "tgt_tokens = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n",
        "\n",
        "# calcule the average attention in the final head\n",
        "att = cross_attentions[-1][0].mean(dim=0).detach().numpy()\n",
        "\n",
        "tensor_1 = np.concatenate((att[0][0:7], att[0][47:-7]), axis=0) # description\n",
        "tensor_2 = att[0][15:37] # possible actions\n",
        "tensor_3 = np.concatenate((att[0][7:15], att[0][37:47]),axis=0) # goal\n",
        "tensor_4 = att[0][-7:-1] # action\n",
        "\n",
        "tensor_1_mean = np.mean(tensor_1)\n",
        "tensor_2_mean = np.mean(tensor_2)\n",
        "tensor_3_mean = np.mean(tensor_3)\n",
        "tensor_4_mean = np.mean(tensor_4)\n",
        "\n",
        "sum_tensor = tensor_1_mean + tensor_2_mean + tensor_3_mean\n",
        "\n",
        "list_attention3 = np.stack([tensor_1_mean, tensor_2_mean, tensor_3_mean])/sum_tensor\n",
        "names_abscisse3 = ['Observation','Possible Actions','Goal']"
      ],
      "metadata": {
        "id": "xfF_yxHE9A6a"
      },
      "execution_count": 443,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatterpolar(\n",
        "    r=list_attention3,\n",
        "    theta=names_abscisse3,\n",
        "    fill='toself',\n",
        "    name='Attention',\n",
        "    line=dict(color='royalblue', width=2)\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(\n",
        "        text=\"Spider Graph of Attention\",\n",
        "        x=0.5,\n",
        "        xanchor='center',\n",
        "        font=dict(size=20, family=\"Arial\", color=\"black\")\n",
        "    ),\n",
        "    polar=dict(\n",
        "        bgcolor=\"rgba(0,0,0,0)\",\n",
        "        radialaxis=dict(\n",
        "            visible=True,\n",
        "            range=[0, 1],\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray',\n",
        "            gridwidth=1,\n",
        "            linewidth=2,\n",
        "            linecolor='gray'\n",
        "        ),\n",
        "        angularaxis=dict(\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray'\n",
        "        )\n",
        "    ),\n",
        "    showlegend=False,\n",
        "    paper_bgcolor=\"white\",\n",
        "    font=dict(family=\"Arial\", size=14, color=\"black\")\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "QcGID6v8C3E4",
        "outputId": "857b8382-7f76-4380-c3c3-72ae0f83f99c"
      },
      "execution_count": 444,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"c01c34f5-0c63-4a73-ad16-c9c118f3fec7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c01c34f5-0c63-4a73-ad16-c9c118f3fec7\")) {                    Plotly.newPlot(                        \"c01c34f5-0c63-4a73-ad16-c9c118f3fec7\",                        [{\"fill\":\"toself\",\"line\":{\"color\":\"royalblue\",\"width\":2},\"name\":\"Attention\",\"r\":[0.64148057,0.00076616404,0.3577533],\"theta\":[\"Observation\",\"Possible Actions\",\"Goal\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"font\":{\"size\":20,\"family\":\"Arial\",\"color\":\"black\"},\"text\":\"Spider Graph of Attention\",\"x\":0.5,\"xanchor\":\"center\"},\"polar\":{\"radialaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"visible\":true,\"range\":[0,1],\"gridcolor\":\"lightgray\",\"gridwidth\":1,\"linewidth\":2,\"linecolor\":\"gray\"},\"angularaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"gridcolor\":\"lightgray\"},\"bgcolor\":\"rgba(0,0,0,0)\"},\"font\":{\"family\":\"Arial\",\"size\":14,\"color\":\"black\"},\"showlegend\":false,\"paper_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c01c34f5-0c63-4a73-ad16-c9c118f3fec7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_attention = [list_attention0, list_attention1, list_attention2, list_attention3]\n",
        "labels = [names_abscisse0, names_abscisse1, names_abscisse2, names_abscisse3]\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Suppose you have four lists of radial data and four lists of corresponding theta labels:\n",
        "# list_attention = [list_attention0, list_attention1, list_attention2, list_attention3]\n",
        "# labels = [names_abscisse0, names_abscisse1, names_abscisse2, names_abscisse3]\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Define some colors for the traces\n",
        "colors = ['royalblue', 'red', 'green', 'orange']\n",
        "\n",
        "# Loop over each set and add it as a trace to the same figure\n",
        "for i in range(len(list_attention)):\n",
        "    fig.add_trace(go.Scatterpolar(\n",
        "        r=list_attention[i],\n",
        "        theta=labels[i],\n",
        "        fill='toself',\n",
        "        name=f'P_{i}',\n",
        "        line=dict(color=colors[i], width=2)\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(\n",
        "        text=\"Superposed Spider Graph\",\n",
        "        x=0.5,\n",
        "        xanchor='center',\n",
        "        font=dict(size=20, family=\"Arial\", color=\"black\")\n",
        "    ),\n",
        "    polar=dict(\n",
        "        bgcolor=\"rgba(0,0,0,0)\",\n",
        "        radialaxis=dict(\n",
        "            visible=True,\n",
        "            range=[0, 1],\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray',\n",
        "            gridwidth=1,\n",
        "            linewidth=2,\n",
        "            linecolor='gray'\n",
        "        ),\n",
        "        angularaxis=dict(\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray'\n",
        "        )\n",
        "    ),\n",
        "    showlegend=True,\n",
        "    paper_bgcolor=\"white\",\n",
        "    font=dict(family=\"Arial\", size=14, color=\"black\")\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "B9sMQewXFrBr",
        "outputId": "1cad8091-a98e-42df-e3e3-693f4dd42a64"
      },
      "execution_count": 445,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"f59b4606-ed0a-401e-a1fc-865ebefdf65b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f59b4606-ed0a-401e-a1fc-865ebefdf65b\")) {                    Plotly.newPlot(                        \"f59b4606-ed0a-401e-a1fc-865ebefdf65b\",                        [{\"fill\":\"toself\",\"line\":{\"color\":\"royalblue\",\"width\":2},\"name\":\"P_0\",\"r\":[0.19486354,0.23196857,0.57316786],\"theta\":[\"Possible Actions\",\"Goal\",\"Observation\"],\"type\":\"scatterpolar\"},{\"fill\":\"toself\",\"line\":{\"color\":\"red\",\"width\":2},\"name\":\"P_1\",\"r\":[0.21381216,0.24743974,0.5387481],\"theta\":[\"Possible Actions\",\"Goal\",\"Observation\"],\"type\":\"scatterpolar\"},{\"fill\":\"toself\",\"line\":{\"color\":\"green\",\"width\":2},\"name\":\"P_2\",\"r\":[0.2968205,0.3095703,0.39360914],\"theta\":[\"Goal\",\"Possible Actions\",\"Observation\"],\"type\":\"scatterpolar\"},{\"fill\":\"toself\",\"line\":{\"color\":\"orange\",\"width\":2},\"name\":\"P_3\",\"r\":[0.64148057,0.00076616404,0.3577533],\"theta\":[\"Observation\",\"Possible Actions\",\"Goal\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"font\":{\"size\":20,\"family\":\"Arial\",\"color\":\"black\"},\"text\":\"Superposed Spider Graph\",\"x\":0.5,\"xanchor\":\"center\"},\"polar\":{\"radialaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"visible\":true,\"range\":[0,1],\"gridcolor\":\"lightgray\",\"gridwidth\":1,\"linewidth\":2,\"linecolor\":\"gray\"},\"angularaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"gridcolor\":\"lightgray\"},\"bgcolor\":\"rgba(0,0,0,0)\"},\"font\":{\"family\":\"Arial\",\"size\":14,\"color\":\"black\"},\"showlegend\":true,\"paper_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f59b4606-ed0a-401e-a1fc-865ebefdf65b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NON COntrastive ( 2nd setting )"
      ],
      "metadata": {
        "id": "PEcTBGJ5yPxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To check if it fits the configuration of our files\n",
        "\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "model1 = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q\", \"k\", \"v\", \"wi\", \"wo\", \"lm_head\"],\n",
        "    lora_dropout=0.0,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model1, lora_config)\n",
        "state_dict = torch.load(\"optimizerNONCONSTRASTIVE.checkpoint\", map_location=\"cpu\")\n",
        "state_dict = {k.replace('_LLM_model.', '').replace('module.', ''): v for k, v in state_dict.items()}\n",
        "\n",
        "# Now try loading the state dict with strict=False\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.config.output_attentions = True\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APg63rqmyPEK",
        "outputId": "c91746b3-b49d-4879-808d-9f7df408ddd5"
      },
      "execution_count": 465,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForSeq2SeqLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): T5ForConditionalGeneration(\n",
              "      (shared): Embedding(32128, 512)\n",
              "      (encoder): T5Stack(\n",
              "        (embed_tokens): Embedding(32128, 512)\n",
              "        (block): ModuleList(\n",
              "          (0): T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                  (relative_attention_bias): Embedding(32, 6)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wo): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1-7): 7 x T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wo): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_layer_norm): T5LayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (decoder): T5Stack(\n",
              "        (embed_tokens): Embedding(32128, 512)\n",
              "        (block): ModuleList(\n",
              "          (0): T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                  (relative_attention_bias): Embedding(32, 6)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerCrossAttention(\n",
              "                (EncDecAttention): T5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (2): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wo): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1-7): 7 x T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerCrossAttention(\n",
              "                (EncDecAttention): T5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (2): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wo): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_layer_norm): T5LayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (lm_head): lora.Linear(\n",
              "        (base_layer): Linear(in_features=512, out_features=32128, bias=False)\n",
              "        (lora_dropout): ModuleDict(\n",
              "          (default): Identity()\n",
              "        )\n",
              "        (lora_A): ModuleDict(\n",
              "          (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "        )\n",
              "        (lora_B): ModuleDict(\n",
              "          (default): Linear(in_features=32, out_features=32128, bias=False)\n",
              "        )\n",
              "        (lora_embedding_A): ParameterDict()\n",
              "        (lora_embedding_B): ParameterDict()\n",
              "        (lora_magnitude_vector): ModuleDict()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 465
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Goal of the agent: go to the purple box \\\n",
        " Possible actions of the agent: turn left, turn right, go forward, pick up, drop, toggle \\\n",
        " Observation: You see a wall 2 steps forward, You see a purple box 2 steps left, You \\\n",
        " see a purple ball 1 step right and 1 step forward, You \\\n",
        " see a grey key 2 steps right Next action :\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Ajout minimal d'un token au dcodeur\n",
        "decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        decoder_input_ids=decoder_input_ids,\n",
        "        output_attentions=True,\n",
        "        return_dict=True\n",
        "    )\n",
        "# Extract attention\n",
        "decoder_attentions = outputs.decoder_attentions  # (num_layers, batch, heads, tgt_len, tgt_len)\n",
        "encoder_attentions = outputs.encoder_attentions  # (num_layers, batch, heads, src_len, src_len)\n",
        "cross_attentions   = outputs.cross_attentions  # (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "src_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "tgt_tokens = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n",
        "\n",
        "# calcule the average attention in the final head\n",
        "att = cross_attentions[-1][0].mean(dim=0).detach().numpy()\n",
        "\n",
        "tensor_1 = att[0][0:10]\n",
        "tensor_2 = att[0][10:32]\n",
        "tensor_3 = att[0][32:-5]\n",
        "tensor_4 = att[0][-5:-1]\n",
        "\n",
        "tensor_1_mean = np.mean(tensor_1)\n",
        "tensor_2_mean = np.mean(tensor_2)\n",
        "tensor_3_mean = np.mean(tensor_3)\n",
        "tensor_4_mean = np.mean(tensor_4)\n",
        "\n",
        "sum_tensor = tensor_1_mean + tensor_2_mean + tensor_3_mean\n",
        "\n",
        "list_attention0 = np.stack([tensor_1_mean, tensor_2_mean, tensor_3_mean])/sum_tensor\n",
        "names_abscisse0 = ['Goal','Possible Actions','Observation']"
      ],
      "metadata": {
        "id": "vjWVsOKsx4Ng"
      },
      "execution_count": 466,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatterpolar(\n",
        "    r=list_attention0,\n",
        "    theta=names_abscisse0,\n",
        "    fill='toself',\n",
        "    name='Attention',\n",
        "    line=dict(color='royalblue', width=2)\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(\n",
        "        text=\"Spider Graph of Attention\",\n",
        "        x=0.5,\n",
        "        xanchor='center',\n",
        "        font=dict(size=20, family=\"Arial\", color=\"black\")\n",
        "    ),\n",
        "    polar=dict(\n",
        "        bgcolor=\"rgba(0,0,0,0)\",\n",
        "        radialaxis=dict(\n",
        "            visible=True,\n",
        "            range=[0, 1],\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray',\n",
        "            gridwidth=1,\n",
        "            linewidth=2,\n",
        "            linecolor='gray'\n",
        "        ),\n",
        "        angularaxis=dict(\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray'\n",
        "        )\n",
        "    ),\n",
        "    showlegend=False,\n",
        "    paper_bgcolor=\"white\",\n",
        "    font=dict(family=\"Arial\", size=14, color=\"black\")\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "4R4-EULRy6rd",
        "outputId": "8382f6fa-b6a0-4a24-9fef-6bf568c66592"
      },
      "execution_count": 467,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"b415f86c-9e04-4fc4-be61-f4feff5d7cc2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b415f86c-9e04-4fc4-be61-f4feff5d7cc2\")) {                    Plotly.newPlot(                        \"b415f86c-9e04-4fc4-be61-f4feff5d7cc2\",                        [{\"fill\":\"toself\",\"line\":{\"color\":\"royalblue\",\"width\":2},\"name\":\"Attention\",\"r\":[0.41678506,0.31825548,0.26495942],\"theta\":[\"Goal\",\"Possible Actions\",\"Observation\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"font\":{\"size\":20,\"family\":\"Arial\",\"color\":\"black\"},\"text\":\"Spider Graph of Attention\",\"x\":0.5,\"xanchor\":\"center\"},\"polar\":{\"radialaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"visible\":true,\"range\":[0,1],\"gridcolor\":\"lightgray\",\"gridwidth\":1,\"linewidth\":2,\"linecolor\":\"gray\"},\"angularaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"gridcolor\":\"lightgray\"},\"bgcolor\":\"rgba(0,0,0,0)\"},\"font\":{\"family\":\"Arial\",\"size\":14,\"color\":\"black\"},\"showlegend\":false,\"paper_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b415f86c-9e04-4fc4-be61-f4feff5d7cc2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Possible actions of the agent: turn left, turn right, go forward, pick up, drop,toggle \\\n",
        "Goal of the agent: go to the purple box \\\n",
        "Observation: You see a wall 2 steps forward, You see a purple box 2 steps left, You \\\n",
        "see a purple ball 1 step right and 1 step forward, You\\\n",
        "see a grey key 2 steps right Next action :\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Ajout minimal d'un token au dcodeur\n",
        "decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        decoder_input_ids=decoder_input_ids,\n",
        "        output_attentions=True,\n",
        "        return_dict=True\n",
        "    )\n",
        "# Extract attention\n",
        "decoder_attentions = outputs.decoder_attentions  # (num_layers, batch, heads, tgt_len, tgt_len)\n",
        "encoder_attentions = outputs.encoder_attentions  # (num_layers, batch, heads, src_len, src_len)\n",
        "cross_attentions   = outputs.cross_attentions  # (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "src_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "tgt_tokens = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n",
        "\n",
        "# calcule the average attention in the final head\n",
        "att = cross_attentions[-1][0].mean(dim=0).detach().numpy()\n",
        "\n",
        "tensor_1 = att[0][:23]\n",
        "tensor_2 = att[0][23:34]\n",
        "tensor_3 = att[0][34:-5]\n",
        "tensor_4 = att[0][-5:-1]\n",
        "\n",
        "tensor_1_mean = np.mean(tensor_1)\n",
        "tensor_2_mean = np.mean(tensor_2)\n",
        "tensor_3_mean = np.mean(tensor_3)\n",
        "tensor_4_mean = np.mean(tensor_4)\n",
        "\n",
        "sum_tensor = tensor_1_mean + tensor_2_mean + tensor_3_mean\n",
        "\n",
        "list_attention1 = np.stack([tensor_1_mean, tensor_2_mean, tensor_3_mean])/sum_tensor\n",
        "names_abscisse1 = ['Possible Actions', 'Goal', 'Observation']"
      ],
      "metadata": {
        "id": "i8YDYd2Rzbgc"
      },
      "execution_count": 468,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatterpolar(\n",
        "    r=list_attention1,\n",
        "    theta=names_abscisse1,\n",
        "    fill='toself',\n",
        "    name='Attention',\n",
        "    line=dict(color='royalblue', width=2)\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(\n",
        "        text=\"Spider Graph of Attention\",\n",
        "        x=0.5,\n",
        "        xanchor='center',\n",
        "        font=dict(size=20, family=\"Arial\", color=\"black\")\n",
        "    ),\n",
        "    polar=dict(\n",
        "        bgcolor=\"rgba(0,0,0,0)\",\n",
        "        radialaxis=dict(\n",
        "            visible=True,\n",
        "            range=[0, 1],\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray',\n",
        "            gridwidth=1,\n",
        "            linewidth=2,\n",
        "            linecolor='gray'\n",
        "        ),\n",
        "        angularaxis=dict(\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray'\n",
        "        )\n",
        "    ),\n",
        "    showlegend=False,\n",
        "    paper_bgcolor=\"white\",\n",
        "    font=dict(family=\"Arial\", size=14, color=\"black\")\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "nD8So42Dzgwx",
        "outputId": "44c5077d-153a-4e7d-ccf8-feae13246b81"
      },
      "execution_count": 469,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"2d4b7bf6-172a-472d-9d6c-7831f7ead190\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2d4b7bf6-172a-472d-9d6c-7831f7ead190\")) {                    Plotly.newPlot(                        \"2d4b7bf6-172a-472d-9d6c-7831f7ead190\",                        [{\"fill\":\"toself\",\"line\":{\"color\":\"royalblue\",\"width\":2},\"name\":\"Attention\",\"r\":[0.20895138,0.33802134,0.4530273],\"theta\":[\"Possible Actions\",\"Goal\",\"Observation\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"font\":{\"size\":20,\"family\":\"Arial\",\"color\":\"black\"},\"text\":\"Spider Graph of Attention\",\"x\":0.5,\"xanchor\":\"center\"},\"polar\":{\"radialaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"visible\":true,\"range\":[0,1],\"gridcolor\":\"lightgray\",\"gridwidth\":1,\"linewidth\":2,\"linecolor\":\"gray\"},\"angularaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"gridcolor\":\"lightgray\"},\"bgcolor\":\"rgba(0,0,0,0)\"},\"font\":{\"family\":\"Arial\",\"size\":14,\"color\":\"black\"},\"showlegend\":false,\"paper_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2d4b7bf6-172a-472d-9d6c-7831f7ead190');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"<Begin.Possible actions>turn left, turn right, go forward, pick up, drop, toggle <End \\\n",
        " Possible actions> \\\n",
        " <Begin Goal> go to a grey box<End Goal>\\\n",
        " <Begin Current Observation>\\\n",
        " Observation: You see a wall 3 steps forward, You see a wall 2 steps\\\n",
        " left, You see a grey ball 1 step right and 1 step forward, You see a\\\n",
        " grey box 2 steps right and 1 step forward, You see a grey box 3 steps\\\n",
        " right and 1 step forward<End Current Observation>\\\n",
        " Next action :\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Ajout minimal d'un token au dcodeur\n",
        "decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        decoder_input_ids=decoder_input_ids,\n",
        "        output_attentions=True,\n",
        "        return_dict=True\n",
        "    )\n",
        "# Extract attention\n",
        "decoder_attentions = outputs.decoder_attentions  # (num_layers, batch, heads, tgt_len, tgt_len)\n",
        "encoder_attentions = outputs.encoder_attentions  # (num_layers, batch, heads, src_len, src_len)\n",
        "cross_attentions   = outputs.cross_attentions  # (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "src_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "tgt_tokens = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n",
        "\n",
        "# calcule the average attention in the final head\n",
        "att = cross_attentions[-1][0].mean(dim=0).detach().numpy()\n",
        "\n",
        "tensor_1 = att[0][:23]\n",
        "tensor_2 = att[0][23:34]\n",
        "tensor_3 = att[0][34:-5]\n",
        "tensor_4 = att[0][-5:-1]\n",
        "\n",
        "tensor_1_mean = np.mean(tensor_1)\n",
        "tensor_2_mean = np.mean(tensor_2)\n",
        "tensor_3_mean = np.mean(tensor_3)\n",
        "tensor_4_mean = np.mean(tensor_4)\n",
        "\n",
        "sum_tensor = tensor_1_mean + tensor_2_mean + tensor_3_mean\n",
        "\n",
        "list_attention2 = np.stack([tensor_1_mean, tensor_2_mean, tensor_3_mean])/sum_tensor\n",
        "names_abscisse2 = ['Possible Actions', 'Goal', 'Observation']"
      ],
      "metadata": {
        "id": "AcWBG1hD6hl8"
      },
      "execution_count": 470,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatterpolar(\n",
        "    r=list_attention2,\n",
        "    theta=names_abscisse2,\n",
        "    fill='toself',\n",
        "    name='Attention',\n",
        "    line=dict(color='royalblue', width=2)\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(\n",
        "        text=\"Spider Graph of Attention\",\n",
        "        x=0.5,\n",
        "        xanchor='center',\n",
        "        font=dict(size=20, family=\"Arial\", color=\"black\")\n",
        "    ),\n",
        "    polar=dict(\n",
        "        bgcolor=\"rgba(0,0,0,0)\",\n",
        "        radialaxis=dict(\n",
        "            visible=True,\n",
        "            range=[0, 1],\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray',\n",
        "            gridwidth=1,\n",
        "            linewidth=2,\n",
        "            linecolor='gray'\n",
        "        ),\n",
        "        angularaxis=dict(\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray'\n",
        "        )\n",
        "    ),\n",
        "    showlegend=False,\n",
        "    paper_bgcolor=\"white\",\n",
        "    font=dict(family=\"Arial\", size=14, color=\"black\")\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "AlU9HDgm6nHs",
        "outputId": "8ea96012-0867-4148-e144-8bd52cc566ff"
      },
      "execution_count": 471,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"dca6a0a9-3962-4cb8-bf6c-16b2c6857f44\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"dca6a0a9-3962-4cb8-bf6c-16b2c6857f44\")) {                    Plotly.newPlot(                        \"dca6a0a9-3962-4cb8-bf6c-16b2c6857f44\",                        [{\"fill\":\"toself\",\"line\":{\"color\":\"royalblue\",\"width\":2},\"name\":\"Attention\",\"r\":[0.20970528,0.5527451,0.23754962],\"theta\":[\"Possible Actions\",\"Goal\",\"Observation\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"font\":{\"size\":20,\"family\":\"Arial\",\"color\":\"black\"},\"text\":\"Spider Graph of Attention\",\"x\":0.5,\"xanchor\":\"center\"},\"polar\":{\"radialaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"visible\":true,\"range\":[0,1],\"gridcolor\":\"lightgray\",\"gridwidth\":1,\"linewidth\":2,\"linecolor\":\"gray\"},\"angularaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"gridcolor\":\"lightgray\"},\"bgcolor\":\"rgba(0,0,0,0)\"},\"font\":{\"family\":\"Arial\",\"size\":14,\"color\":\"black\"},\"showlegend\":false,\"paper_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('dca6a0a9-3962-4cb8-bf6c-16b2c6857f44');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \" you are on a maze and you have to solve a task, what you can do is: turn left, turn \\\n",
        " right, go forward, pick up, drop, toggle your task is to go to a grey box, what you see \\\n",
        " now: You see a wall 3 steps forward, You see a wall 2 steps \\\n",
        " left, You see a grey ball 1 step right and 1 step forward, You see a grey box 2 steps\\\n",
        " right and 1 step forward, You see a grey box 3 steps right and 1 step forward\\\n",
        " and you next action is to\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Ajout minimal d'un token au dcodeur\n",
        "decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        decoder_input_ids=decoder_input_ids,\n",
        "        output_attentions=True,\n",
        "        return_dict=True\n",
        "    )\n",
        "# Extract attention\n",
        "decoder_attentions = outputs.decoder_attentions  # (num_layers, batch, heads, tgt_len, tgt_len)\n",
        "encoder_attentions = outputs.encoder_attentions  # (num_layers, batch, heads, src_len, src_len)\n",
        "cross_attentions   = outputs.cross_attentions  # (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "src_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "tgt_tokens = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n",
        "\n",
        "# calcule the average attention in the final head\n",
        "att = cross_attentions[-1][0].mean(dim=0).detach().numpy()\n",
        "\n",
        "tensor_1 = np.concatenate((att[0][0:7], att[0][47:-7]), axis=0) # description\n",
        "tensor_2 = att[0][15:37] # possible actions\n",
        "tensor_3 = np.concatenate((att[0][7:15], att[0][37:47]),axis=0) # goal\n",
        "tensor_4 = att[0][-7:-1] # action\n",
        "\n",
        "tensor_1_mean = np.mean(tensor_1)\n",
        "tensor_2_mean = np.mean(tensor_2)\n",
        "tensor_3_mean = np.mean(tensor_3)\n",
        "tensor_4_mean = np.mean(tensor_4)\n",
        "\n",
        "sum_tensor = tensor_1_mean + tensor_2_mean + tensor_3_mean\n",
        "\n",
        "list_attention3 = np.stack([tensor_1_mean, tensor_2_mean, tensor_3_mean])/sum_tensor\n",
        "names_abscisse3 = ['Goal','Possible Actions','Observation']"
      ],
      "metadata": {
        "id": "8u-Yj10hC-hB"
      },
      "execution_count": 472,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatterpolar(\n",
        "    r=list_attention3,\n",
        "    theta=names_abscisse3,\n",
        "    fill='toself',\n",
        "    name='Attention',\n",
        "    line=dict(color='royalblue', width=2)\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(\n",
        "        text=\"Spider Graph of Attention\",\n",
        "        x=0.5,\n",
        "        xanchor='center',\n",
        "        font=dict(size=20, family=\"Arial\", color=\"black\")\n",
        "    ),\n",
        "    polar=dict(\n",
        "        bgcolor=\"rgba(0,0,0,0)\",\n",
        "        radialaxis=dict(\n",
        "            visible=True,\n",
        "            range=[0, 1],\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray',\n",
        "            gridwidth=1,\n",
        "            linewidth=2,\n",
        "            linecolor='gray'\n",
        "        ),\n",
        "        angularaxis=dict(\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray'\n",
        "        )\n",
        "    ),\n",
        "    showlegend=False,\n",
        "    paper_bgcolor=\"white\",\n",
        "    font=dict(family=\"Arial\", size=14, color=\"black\")\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "bo4-GqMRDDw6",
        "outputId": "44b4d71b-81f4-4bf9-b1e5-8761c6237f48"
      },
      "execution_count": 473,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"aefb6a62-4287-4d43-ad4b-a3e1a5ab24f8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"aefb6a62-4287-4d43-ad4b-a3e1a5ab24f8\")) {                    Plotly.newPlot(                        \"aefb6a62-4287-4d43-ad4b-a3e1a5ab24f8\",                        [{\"fill\":\"toself\",\"line\":{\"color\":\"royalblue\",\"width\":2},\"name\":\"Attention\",\"r\":[0.377988,0.08581558,0.5361964],\"theta\":[\"Goal\",\"Possible Actions\",\"Observation\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"font\":{\"size\":20,\"family\":\"Arial\",\"color\":\"black\"},\"text\":\"Spider Graph of Attention\",\"x\":0.5,\"xanchor\":\"center\"},\"polar\":{\"radialaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"visible\":true,\"range\":[0,1],\"gridcolor\":\"lightgray\",\"gridwidth\":1,\"linewidth\":2,\"linecolor\":\"gray\"},\"angularaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"gridcolor\":\"lightgray\"},\"bgcolor\":\"rgba(0,0,0,0)\"},\"font\":{\"family\":\"Arial\",\"size\":14,\"color\":\"black\"},\"showlegend\":false,\"paper_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('aefb6a62-4287-4d43-ad4b-a3e1a5ab24f8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_attention = [list_attention0, list_attention1, list_attention2, list_attention3]\n",
        "labels = [names_abscisse0, names_abscisse1, names_abscisse2, names_abscisse3]\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Suppose you have four lists of radial data and four lists of corresponding theta labels:\n",
        "# list_attention = [list_attention0, list_attention1, list_attention2, list_attention3]\n",
        "# labels = [names_abscisse0, names_abscisse1, names_abscisse2, names_abscisse3]\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Define some colors for the traces\n",
        "colors = ['royalblue', 'red', 'green', 'orange']\n",
        "\n",
        "# Loop over each set and add it as a trace to the same figure\n",
        "for i in range(len(list_attention)):\n",
        "    fig.add_trace(go.Scatterpolar(\n",
        "        r=list_attention[i],\n",
        "        theta=labels[i],\n",
        "        fill='toself',\n",
        "        name=f'P_{i}',\n",
        "        line=dict(color=colors[i], width=2)\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(\n",
        "        text=\"Superposed Spider Graph\",\n",
        "        x=0.5,\n",
        "        xanchor='center',\n",
        "        font=dict(size=20, family=\"Arial\", color=\"black\")\n",
        "    ),\n",
        "    polar=dict(\n",
        "        bgcolor=\"rgba(0,0,0,0)\",\n",
        "        radialaxis=dict(\n",
        "            visible=True,\n",
        "            range=[0, 1],\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray',\n",
        "            gridwidth=1,\n",
        "            linewidth=2,\n",
        "            linecolor='gray'\n",
        "        ),\n",
        "        angularaxis=dict(\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray'\n",
        "        )\n",
        "    ),\n",
        "    showlegend=True,\n",
        "    paper_bgcolor=\"white\",\n",
        "    font=dict(family=\"Arial\", size=14, color=\"black\")\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "rKmjPf0HJ--p",
        "outputId": "805f2310-e79f-428c-b868-3db64f73b7da"
      },
      "execution_count": 474,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"b2e06be2-e3b3-43dc-baea-81de470b96e7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b2e06be2-e3b3-43dc-baea-81de470b96e7\")) {                    Plotly.newPlot(                        \"b2e06be2-e3b3-43dc-baea-81de470b96e7\",                        [{\"fill\":\"toself\",\"line\":{\"color\":\"royalblue\",\"width\":2},\"name\":\"P_0\",\"r\":[0.41678506,0.31825548,0.26495942],\"theta\":[\"Goal\",\"Possible Actions\",\"Observation\"],\"type\":\"scatterpolar\"},{\"fill\":\"toself\",\"line\":{\"color\":\"red\",\"width\":2},\"name\":\"P_1\",\"r\":[0.20895138,0.33802134,0.4530273],\"theta\":[\"Possible Actions\",\"Goal\",\"Observation\"],\"type\":\"scatterpolar\"},{\"fill\":\"toself\",\"line\":{\"color\":\"green\",\"width\":2},\"name\":\"P_2\",\"r\":[0.20970528,0.5527451,0.23754962],\"theta\":[\"Possible Actions\",\"Goal\",\"Observation\"],\"type\":\"scatterpolar\"},{\"fill\":\"toself\",\"line\":{\"color\":\"orange\",\"width\":2},\"name\":\"P_3\",\"r\":[0.377988,0.08581558,0.5361964],\"theta\":[\"Goal\",\"Possible Actions\",\"Observation\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"font\":{\"size\":20,\"family\":\"Arial\",\"color\":\"black\"},\"text\":\"Superposed Spider Graph\",\"x\":0.5,\"xanchor\":\"center\"},\"polar\":{\"radialaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"visible\":true,\"range\":[0,1],\"gridcolor\":\"lightgray\",\"gridwidth\":1,\"linewidth\":2,\"linecolor\":\"gray\"},\"angularaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"gridcolor\":\"lightgray\"},\"bgcolor\":\"rgba(0,0,0,0)\"},\"font\":{\"family\":\"Arial\",\"size\":14,\"color\":\"black\"},\"showlegend\":true,\"paper_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b2e06be2-e3b3-43dc-baea-81de470b96e7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First setting : sigma_0"
      ],
      "metadata": {
        "id": "qVXlXlZf-A5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To check if it fits the configuration of our files\n",
        "\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "model1 = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q\", \"k\", \"v\", \"wi\", \"wo\", \"lm_head\"],\n",
        "    lora_dropout=0.0,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model1, lora_config)\n",
        "state_dict = torch.load(\"model_PO.checkpoint\", map_location=\"cpu\")\n",
        "state_dict = {k.replace('_LLM_model.', '').replace('module.', ''): v for k, v in state_dict.items()}\n",
        "\n",
        "# Now try loading the state dict with strict=False\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.config.output_attentions = True\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyVYZE-g-E86",
        "outputId": "b5100711-f741-459e-b4d0-eb10e7210e5d"
      },
      "execution_count": 475,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForSeq2SeqLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): T5ForConditionalGeneration(\n",
              "      (shared): Embedding(32128, 512)\n",
              "      (encoder): T5Stack(\n",
              "        (embed_tokens): Embedding(32128, 512)\n",
              "        (block): ModuleList(\n",
              "          (0): T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                  (relative_attention_bias): Embedding(32, 6)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wo): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1-7): 7 x T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wo): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_layer_norm): T5LayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (decoder): T5Stack(\n",
              "        (embed_tokens): Embedding(32128, 512)\n",
              "        (block): ModuleList(\n",
              "          (0): T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                  (relative_attention_bias): Embedding(32, 6)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerCrossAttention(\n",
              "                (EncDecAttention): T5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (2): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wo): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1-7): 7 x T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerCrossAttention(\n",
              "                (EncDecAttention): T5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=384, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (2): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                  (wo): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Identity()\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=512, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_layer_norm): T5LayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (lm_head): lora.Linear(\n",
              "        (base_layer): Linear(in_features=512, out_features=32128, bias=False)\n",
              "        (lora_dropout): ModuleDict(\n",
              "          (default): Identity()\n",
              "        )\n",
              "        (lora_A): ModuleDict(\n",
              "          (default): Linear(in_features=512, out_features=32, bias=False)\n",
              "        )\n",
              "        (lora_B): ModuleDict(\n",
              "          (default): Linear(in_features=32, out_features=32128, bias=False)\n",
              "        )\n",
              "        (lora_embedding_A): ParameterDict()\n",
              "        (lora_embedding_B): ParameterDict()\n",
              "        (lora_magnitude_vector): ModuleDict()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 475
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Goal of the agent: go to the purple box \\\n",
        " Possible actions of the agent: turn left, turn right, go forward, pick up, drop, toggle \\\n",
        " Observation: You see a wall 2 steps forward, You see a purple box 2 steps left, You \\\n",
        " see a purple ball 1 step right and 1 step forward, You \\\n",
        " see a grey key 2 steps right Next action :\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Ajout minimal d'un token au dcodeur\n",
        "decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        decoder_input_ids=decoder_input_ids,\n",
        "        output_attentions=True,\n",
        "        return_dict=True\n",
        "    )\n",
        "# Extract attention\n",
        "decoder_attentions = outputs.decoder_attentions  # (num_layers, batch, heads, tgt_len, tgt_len)\n",
        "encoder_attentions = outputs.encoder_attentions  # (num_layers, batch, heads, src_len, src_len)\n",
        "cross_attentions   = outputs.cross_attentions  # (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "src_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "tgt_tokens = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n",
        "\n",
        "# calcule the average attention in the final head\n",
        "att = cross_attentions[-1][0].mean(dim=0).detach().numpy()\n",
        "\n",
        "tensor_1 = att[0][0:10]\n",
        "tensor_2 = att[0][10:32]\n",
        "tensor_3 = att[0][32:-5]\n",
        "tensor_4 = att[0][-5:-1]\n",
        "\n",
        "tensor_1_mean = np.mean(tensor_1)\n",
        "tensor_2_mean = np.mean(tensor_2)\n",
        "tensor_3_mean = np.mean(tensor_3)\n",
        "tensor_4_mean = np.mean(tensor_4)\n",
        "\n",
        "sum_tensor = tensor_1_mean + tensor_2_mean + tensor_3_mean\n",
        "\n",
        "list_attention0 = np.stack([tensor_1_mean, tensor_2_mean, tensor_3_mean])/sum_tensor\n",
        "names_abscisse0 = ['Goal','Possible Actions','Observation']"
      ],
      "metadata": {
        "id": "CIKPhtugDPuR"
      },
      "execution_count": 476,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatterpolar(\n",
        "    r=list_attention0,\n",
        "    theta=names_abscisse0,\n",
        "    fill='toself',\n",
        "    name='Attention',\n",
        "    line=dict(color='royalblue', width=2)\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(\n",
        "        text=\"Spider Graph of Attention\",\n",
        "        x=0.5,\n",
        "        xanchor='center',\n",
        "        font=dict(size=20, family=\"Arial\", color=\"black\")\n",
        "    ),\n",
        "    polar=dict(\n",
        "        bgcolor=\"rgba(0,0,0,0)\",\n",
        "        radialaxis=dict(\n",
        "            visible=True,\n",
        "            range=[0, 1],\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray',\n",
        "            gridwidth=1,\n",
        "            linewidth=2,\n",
        "            linecolor='gray'\n",
        "        ),\n",
        "        angularaxis=dict(\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray'\n",
        "        )\n",
        "    ),\n",
        "    showlegend=False,\n",
        "    paper_bgcolor=\"white\",\n",
        "    font=dict(family=\"Arial\", size=14, color=\"black\")\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "xmiNCwn5DYbm",
        "outputId": "6bc8dac4-959e-42f6-bf18-740535e011d0"
      },
      "execution_count": 477,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"c0ce2193-616c-4726-bbf3-cc2ac7bf4b9b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c0ce2193-616c-4726-bbf3-cc2ac7bf4b9b\")) {                    Plotly.newPlot(                        \"c0ce2193-616c-4726-bbf3-cc2ac7bf4b9b\",                        [{\"fill\":\"toself\",\"line\":{\"color\":\"royalblue\",\"width\":2},\"name\":\"Attention\",\"r\":[0.42098975,0.31232467,0.26668555],\"theta\":[\"Goal\",\"Possible Actions\",\"Observation\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"font\":{\"size\":20,\"family\":\"Arial\",\"color\":\"black\"},\"text\":\"Spider Graph of Attention\",\"x\":0.5,\"xanchor\":\"center\"},\"polar\":{\"radialaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"visible\":true,\"range\":[0,1],\"gridcolor\":\"lightgray\",\"gridwidth\":1,\"linewidth\":2,\"linecolor\":\"gray\"},\"angularaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"gridcolor\":\"lightgray\"},\"bgcolor\":\"rgba(0,0,0,0)\"},\"font\":{\"family\":\"Arial\",\"size\":14,\"color\":\"black\"},\"showlegend\":false,\"paper_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c0ce2193-616c-4726-bbf3-cc2ac7bf4b9b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Possible actions of the agent: turn left, turn right, go forward, pick up, drop,toggle \\\n",
        "Goal of the agent: go to the purple box \\\n",
        "Observation: You see a wall 2 steps forward, You see a purple box 2 steps left, You \\\n",
        "see a purple ball 1 step right and 1 step forward, You\\\n",
        "see a grey key 2 steps right Next action :\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Ajout minimal d'un token au dcodeur\n",
        "decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        decoder_input_ids=decoder_input_ids,\n",
        "        output_attentions=True,\n",
        "        return_dict=True\n",
        "    )\n",
        "# Extract attention\n",
        "decoder_attentions = outputs.decoder_attentions  # (num_layers, batch, heads, tgt_len, tgt_len)\n",
        "encoder_attentions = outputs.encoder_attentions  # (num_layers, batch, heads, src_len, src_len)\n",
        "cross_attentions   = outputs.cross_attentions  # (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "src_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "tgt_tokens = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n",
        "\n",
        "# calcule the average attention in the final head\n",
        "att = cross_attentions[-1][0].mean(dim=0).detach().numpy()\n",
        "\n",
        "tensor_1 = att[0][:23]\n",
        "tensor_2 = att[0][23:34]\n",
        "tensor_3 = att[0][34:-5]\n",
        "tensor_4 = att[0][-5:-1]\n",
        "\n",
        "tensor_1_mean = np.mean(tensor_1)\n",
        "tensor_2_mean = np.mean(tensor_2)\n",
        "tensor_3_mean = np.mean(tensor_3)\n",
        "tensor_4_mean = np.mean(tensor_4)\n",
        "\n",
        "sum_tensor = tensor_1_mean + tensor_2_mean + tensor_3_mean\n",
        "\n",
        "list_attention1 = np.stack([tensor_1_mean, tensor_2_mean, tensor_3_mean])/sum_tensor\n",
        "names_abscisse1 = ['Possible Actions', 'Goal', 'Observation','Action']"
      ],
      "metadata": {
        "id": "zIoXD9vmDiRt"
      },
      "execution_count": 478,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatterpolar(\n",
        "    r=list_attention1,\n",
        "    theta=names_abscisse1,\n",
        "    fill='toself',\n",
        "    name='Attention',\n",
        "    line=dict(color='royalblue', width=2)\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(\n",
        "        text=\"Spider Graph of Attention\",\n",
        "        x=0.5,\n",
        "        xanchor='center',\n",
        "        font=dict(size=20, family=\"Arial\", color=\"black\")\n",
        "    ),\n",
        "    polar=dict(\n",
        "        bgcolor=\"rgba(0,0,0,0)\",\n",
        "        radialaxis=dict(\n",
        "            visible=True,\n",
        "            range=[0, 1],\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray',\n",
        "            gridwidth=1,\n",
        "            linewidth=2,\n",
        "            linecolor='gray'\n",
        "        ),\n",
        "        angularaxis=dict(\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray'\n",
        "        )\n",
        "    ),\n",
        "    showlegend=False,\n",
        "    paper_bgcolor=\"white\",\n",
        "    font=dict(family=\"Arial\", size=14, color=\"black\")\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "1Fyxu7yUDnAO",
        "outputId": "b7649eb6-96ea-4ed7-e7a4-21d76c5e748e"
      },
      "execution_count": 479,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"5c2fe3e9-8a60-4756-b192-ffc73531ebbf\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5c2fe3e9-8a60-4756-b192-ffc73531ebbf\")) {                    Plotly.newPlot(                        \"5c2fe3e9-8a60-4756-b192-ffc73531ebbf\",                        [{\"fill\":\"toself\",\"line\":{\"color\":\"royalblue\",\"width\":2},\"name\":\"Attention\",\"r\":[0.19981469,0.34552383,0.45466143],\"theta\":[\"Possible Actions\",\"Goal\",\"Observation\",\"Action\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"font\":{\"size\":20,\"family\":\"Arial\",\"color\":\"black\"},\"text\":\"Spider Graph of Attention\",\"x\":0.5,\"xanchor\":\"center\"},\"polar\":{\"radialaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"visible\":true,\"range\":[0,1],\"gridcolor\":\"lightgray\",\"gridwidth\":1,\"linewidth\":2,\"linecolor\":\"gray\"},\"angularaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"gridcolor\":\"lightgray\"},\"bgcolor\":\"rgba(0,0,0,0)\"},\"font\":{\"family\":\"Arial\",\"size\":14,\"color\":\"black\"},\"showlegend\":false,\"paper_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5c2fe3e9-8a60-4756-b192-ffc73531ebbf');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"<Begin.Possible actions>turn left, turn right, go forward, pick up, drop, toggle <End \\\n",
        " Possible actions> \\\n",
        " <Begin Goal> go to a grey box<End Goal>\\\n",
        " <Begin Current Observation>\\\n",
        " Observation: You see a wall 3 steps forward, You see a wall 2 steps\\\n",
        " left, You see a grey ball 1 step right and 1 step forward, You see a\\\n",
        " grey box 2 steps right and 1 step forward, You see a grey box 3 steps\\\n",
        " right and 1 step forward<End Current Observation>\\\n",
        " Next action :\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Ajout minimal d'un token au dcodeur\n",
        "decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        decoder_input_ids=decoder_input_ids,\n",
        "        output_attentions=True,\n",
        "        return_dict=True\n",
        "    )\n",
        "# Extract attention\n",
        "decoder_attentions = outputs.decoder_attentions  # (num_layers, batch, heads, tgt_len, tgt_len)\n",
        "encoder_attentions = outputs.encoder_attentions  # (num_layers, batch, heads, src_len, src_len)\n",
        "cross_attentions   = outputs.cross_attentions  # (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "src_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "tgt_tokens = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n",
        "\n",
        "# calcule the average attention in the final head\n",
        "att = cross_attentions[-1][0].mean(dim=0).detach().numpy()\n",
        "\n",
        "tensor_1 = att[0][:23]\n",
        "tensor_2 = att[0][23:34]\n",
        "tensor_3 = att[0][34:-5]\n",
        "tensor_4 = att[0][-5:-1]\n",
        "\n",
        "tensor_1_mean = np.mean(tensor_1)\n",
        "tensor_2_mean = np.mean(tensor_2)\n",
        "tensor_3_mean = np.mean(tensor_3)\n",
        "tensor_4_mean = np.mean(tensor_4)\n",
        "\n",
        "sum_tensor = tensor_1_mean + tensor_2_mean + tensor_3_mean\n",
        "\n",
        "list_attention2 = np.stack([tensor_1_mean, tensor_2_mean, tensor_3_mean])/sum_tensor\n",
        "names_abscisse2 = ['Possible Actions', 'Goal', 'Observation']"
      ],
      "metadata": {
        "id": "v4e-bB-TDv9h"
      },
      "execution_count": 480,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatterpolar(\n",
        "    r=list_attention2,\n",
        "    theta=names_abscisse2,\n",
        "    fill='toself',\n",
        "    name='Attention',\n",
        "    line=dict(color='royalblue', width=2)\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(\n",
        "        text=\"Spider Graph of Attention\",\n",
        "        x=0.5,\n",
        "        xanchor='center',\n",
        "        font=dict(size=20, family=\"Arial\", color=\"black\")\n",
        "    ),\n",
        "    polar=dict(\n",
        "        bgcolor=\"rgba(0,0,0,0)\",\n",
        "        radialaxis=dict(\n",
        "            visible=True,\n",
        "            range=[0, 1],\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray',\n",
        "            gridwidth=1,\n",
        "            linewidth=2,\n",
        "            linecolor='gray'\n",
        "        ),\n",
        "        angularaxis=dict(\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray'\n",
        "        )\n",
        "    ),\n",
        "    showlegend=False,\n",
        "    paper_bgcolor=\"white\",\n",
        "    font=dict(family=\"Arial\", size=14, color=\"black\")\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "fsud9sbRDxNc",
        "outputId": "b8ef098d-538c-4e78-ed40-289533819e0a"
      },
      "execution_count": 481,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"f77f9bde-fcbb-4d3a-81da-4823f1639872\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f77f9bde-fcbb-4d3a-81da-4823f1639872\")) {                    Plotly.newPlot(                        \"f77f9bde-fcbb-4d3a-81da-4823f1639872\",                        [{\"fill\":\"toself\",\"line\":{\"color\":\"royalblue\",\"width\":2},\"name\":\"Attention\",\"r\":[0.20850922,0.5521577,0.23933308],\"theta\":[\"Possible Actions\",\"Goal\",\"Observation\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"font\":{\"size\":20,\"family\":\"Arial\",\"color\":\"black\"},\"text\":\"Spider Graph of Attention\",\"x\":0.5,\"xanchor\":\"center\"},\"polar\":{\"radialaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"visible\":true,\"range\":[0,1],\"gridcolor\":\"lightgray\",\"gridwidth\":1,\"linewidth\":2,\"linecolor\":\"gray\"},\"angularaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"gridcolor\":\"lightgray\"},\"bgcolor\":\"rgba(0,0,0,0)\"},\"font\":{\"family\":\"Arial\",\"size\":14,\"color\":\"black\"},\"showlegend\":false,\"paper_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f77f9bde-fcbb-4d3a-81da-4823f1639872');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \" you are on a maze and you have to solve a task, what you can do is: turn left, turn \\\n",
        " right, go forward, pick up, drop, toggle your task is to go to a grey box, what you see \\\n",
        " now: You see a wall 3 steps forward, You see a wall 2 steps \\\n",
        " left, You see a grey ball 1 step right and 1 step forward, You see a grey box 2 steps\\\n",
        " right and 1 step forward, You see a grey box 3 steps right and 1 step forward\\\n",
        " and you next action is to\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Ajout minimal d'un token au dcodeur\n",
        "decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        decoder_input_ids=decoder_input_ids,\n",
        "        output_attentions=True,\n",
        "        return_dict=True\n",
        "    )\n",
        "# Extract attention\n",
        "decoder_attentions = outputs.decoder_attentions  # (num_layers, batch, heads, tgt_len, tgt_len)\n",
        "encoder_attentions = outputs.encoder_attentions  # (num_layers, batch, heads, src_len, src_len)\n",
        "cross_attentions   = outputs.cross_attentions  # (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "src_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "tgt_tokens = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n",
        "\n",
        "# calcule the average attention in the final head\n",
        "att = cross_attentions[-1][0].mean(dim=0).detach().numpy()\n",
        "\n",
        "tensor_1 = np.concatenate((att[0][0:7], att[0][47:-7]), axis=0) # description\n",
        "tensor_2 = att[0][15:37] # possible actions\n",
        "tensor_3 = np.concatenate((att[0][7:15], att[0][37:47]),axis=0) # goal\n",
        "tensor_4 = att[0][-7:-1] # action\n",
        "\n",
        "tensor_1_mean = np.mean(tensor_1)\n",
        "tensor_2_mean = np.mean(tensor_2)\n",
        "tensor_3_mean = np.mean(tensor_3)\n",
        "tensor_4_mean = np.mean(tensor_4)\n",
        "\n",
        "sum_tensor = tensor_1_mean + tensor_2_mean + tensor_3_mean\n",
        "\n",
        "list_attention3 = np.stack([tensor_1_mean, tensor_2_mean, tensor_3_mean])/sum_tensor\n",
        "names_abscisse3 = ['Goal','Possible Actions','Observation','Action']"
      ],
      "metadata": {
        "id": "6vcmVf80D6jN"
      },
      "execution_count": 482,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatterpolar(\n",
        "    r=list_attention3,\n",
        "    theta=names_abscisse3,\n",
        "    fill='toself',\n",
        "    name='Attention',\n",
        "    line=dict(color='royalblue', width=2)\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(\n",
        "        text=\"Spider Graph of Attention\",\n",
        "        x=0.5,\n",
        "        xanchor='center',\n",
        "        font=dict(size=20, family=\"Arial\", color=\"black\")\n",
        "    ),\n",
        "    polar=dict(\n",
        "        bgcolor=\"rgba(0,0,0,0)\",\n",
        "        radialaxis=dict(\n",
        "            visible=True,\n",
        "            range=[0, 1],\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray',\n",
        "            gridwidth=1,\n",
        "            linewidth=2,\n",
        "            linecolor='gray'\n",
        "        ),\n",
        "        angularaxis=dict(\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray'\n",
        "        )\n",
        "    ),\n",
        "    showlegend=False,\n",
        "    paper_bgcolor=\"white\",\n",
        "    font=dict(family=\"Arial\", size=14, color=\"black\")\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "5Kr5-a2QD8z1",
        "outputId": "fbb96ab4-42b0-4dc0-ddb7-73e2d1357b79"
      },
      "execution_count": 483,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"659eb71b-6eb7-4022-b993-5827fd4f29ef\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"659eb71b-6eb7-4022-b993-5827fd4f29ef\")) {                    Plotly.newPlot(                        \"659eb71b-6eb7-4022-b993-5827fd4f29ef\",                        [{\"fill\":\"toself\",\"line\":{\"color\":\"royalblue\",\"width\":2},\"name\":\"Attention\",\"r\":[0.37781847,0.084017314,0.53816426],\"theta\":[\"Goal\",\"Possible Actions\",\"Observation\",\"Action\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"font\":{\"size\":20,\"family\":\"Arial\",\"color\":\"black\"},\"text\":\"Spider Graph of Attention\",\"x\":0.5,\"xanchor\":\"center\"},\"polar\":{\"radialaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"visible\":true,\"range\":[0,1],\"gridcolor\":\"lightgray\",\"gridwidth\":1,\"linewidth\":2,\"linecolor\":\"gray\"},\"angularaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"gridcolor\":\"lightgray\"},\"bgcolor\":\"rgba(0,0,0,0)\"},\"font\":{\"family\":\"Arial\",\"size\":14,\"color\":\"black\"},\"showlegend\":false,\"paper_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('659eb71b-6eb7-4022-b993-5827fd4f29ef');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_attention = [list_attention0, list_attention1, list_attention2, list_attention3]\n",
        "labels = [names_abscisse0, names_abscisse1, names_abscisse2, names_abscisse3]\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Define some colors for the traces\n",
        "colors = ['royalblue', 'red', 'green', 'orange']\n",
        "\n",
        "# Loop over each set and add it as a trace to the same figure\n",
        "for i in range(len(list_attention)):\n",
        "    fig.add_trace(go.Scatterpolar(\n",
        "        r=list_attention[i],\n",
        "        theta=labels[i],\n",
        "        fill='toself',\n",
        "        name=f'P_{i}',\n",
        "        line=dict(color=colors[i], width=2)\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(\n",
        "        text=\"Superposed Spider Graph\",\n",
        "        x=0.5,\n",
        "        xanchor='center',\n",
        "        font=dict(size=20, family=\"Arial\", color=\"black\")\n",
        "    ),\n",
        "    polar=dict(\n",
        "        bgcolor=\"rgba(0,0,0,0)\",\n",
        "        radialaxis=dict(\n",
        "            visible=True,\n",
        "            range=[0, 1],\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray',\n",
        "            gridwidth=1,\n",
        "            linewidth=2,\n",
        "            linecolor='gray'\n",
        "        ),\n",
        "        angularaxis=dict(\n",
        "            tickfont=dict(size=12, color='gray'),\n",
        "            gridcolor='lightgray'\n",
        "        )\n",
        "    ),\n",
        "    showlegend=True,\n",
        "    paper_bgcolor=\"white\",\n",
        "    font=dict(family=\"Arial\", size=14, color=\"black\")\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "f_p9ixpLKnQ7",
        "outputId": "8e3e33f1-5178-4d04-fda0-e711146cc3bf"
      },
      "execution_count": 484,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"1958ab30-1aff-4046-b81e-e99490d8a55f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1958ab30-1aff-4046-b81e-e99490d8a55f\")) {                    Plotly.newPlot(                        \"1958ab30-1aff-4046-b81e-e99490d8a55f\",                        [{\"fill\":\"toself\",\"line\":{\"color\":\"royalblue\",\"width\":2},\"name\":\"P_0\",\"r\":[0.42098975,0.31232467,0.26668555],\"theta\":[\"Goal\",\"Possible Actions\",\"Observation\"],\"type\":\"scatterpolar\"},{\"fill\":\"toself\",\"line\":{\"color\":\"red\",\"width\":2},\"name\":\"P_1\",\"r\":[0.19981469,0.34552383,0.45466143],\"theta\":[\"Possible Actions\",\"Goal\",\"Observation\",\"Action\"],\"type\":\"scatterpolar\"},{\"fill\":\"toself\",\"line\":{\"color\":\"green\",\"width\":2},\"name\":\"P_2\",\"r\":[0.20850922,0.5521577,0.23933308],\"theta\":[\"Possible Actions\",\"Goal\",\"Observation\"],\"type\":\"scatterpolar\"},{\"fill\":\"toself\",\"line\":{\"color\":\"orange\",\"width\":2},\"name\":\"P_3\",\"r\":[0.37781847,0.084017314,0.53816426],\"theta\":[\"Goal\",\"Possible Actions\",\"Observation\",\"Action\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"font\":{\"size\":20,\"family\":\"Arial\",\"color\":\"black\"},\"text\":\"Superposed Spider Graph\",\"x\":0.5,\"xanchor\":\"center\"},\"polar\":{\"radialaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"visible\":true,\"range\":[0,1],\"gridcolor\":\"lightgray\",\"gridwidth\":1,\"linewidth\":2,\"linecolor\":\"gray\"},\"angularaxis\":{\"tickfont\":{\"size\":12,\"color\":\"gray\"},\"gridcolor\":\"lightgray\"},\"bgcolor\":\"rgba(0,0,0,0)\"},\"font\":{\"family\":\"Arial\",\"size\":14,\"color\":\"black\"},\"showlegend\":true,\"paper_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1958ab30-1aff-4046-b81e-e99490d8a55f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}